{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbzZLqIPv6b7",
        "outputId": "19f2fc2b-6f1d-4b43-fd50-4c513e3936fd"
      },
      "source": [
        "# Transformer Architecture Exploration\n",
        "\n",
        "In this notebook, I explore the Transformer architecture with a focus on its core building blocks. Transformer’s have the ability to process sequences in parallel, which is unique from the sequential nature of RNNs. This offers major improvements in training speed and scalability for deep learning tasks involving text, time series, and more.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "- Understand how positional encodings allow the Transformer model to incorporate token order\n",
        "- Implement the scaled dot-product attention mechanism\n",
        "- Explore how multi-head attention improves representation learning\n",
        "- Assemble a basic Transformer encoder-decoder model from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSE8y57S_4ID"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OpwqWL2QH5G"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "# add src folder to path (will possibly make as package in future)\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
        "from tests.architecture_tests import get_angles_test, positional_encoding_test, scaled_dot_product_attention_test, EncoderLayer_test, DecoderLayer_test, Decoder_test, Transformer_test\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
        "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
        "from transformers import TFDistilBertForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hmxf3h5BfO_"
      },
      "outputs": [],
      "source": [
        "SEED = 10\n",
        "# TensorFlow random seed\n",
        "tf.random.set_seed(SEED)\n",
        "# NumPy random seed\n",
        "np.random.seed(SEED)\n",
        "# Python random seed\n",
        "random.seed(SEED)\n",
        "# deterministic operations (for GPU computations)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smPQmicy_4IE"
      },
      "source": [
        "## 1. Positional Encoding\n",
        "\n",
        "Unlike RNNs, Transformer models process entire input sequences at the same time, which essentially means they don’t understand the order of tokens. To overcome this issue, **positional encodings** are used to inject position-specific information into the input embeddings.\n",
        "\n",
        "A common approach for this involves using sine and cosine functions of varying frequencies, which allow the model to learn relationships between token positions in a continuous, smooth space.\n",
        "\n",
        "These encodings are calculated as follows:\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right) \\quad (1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right) \\quad (2)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $pos$ is the token position in the sequence\n",
        "- $i$ is the dimension index\n",
        "- $d$ is the total dimensionality of the embedding\n",
        "\n",
        "By adding these position-based vectors to the word embeddings, semantic meaning is preserved and order related information can be introduced. The encoding also makes sure that relative distances between positions are represented consistently across dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "## Understanding the Angle Formula\n",
        "\n",
        "Both sine and cosine encodings share a common angle term:\n",
        "\n",
        "$$\n",
        "\\theta(pos, i, d) = \\frac{pos}{10000^{\\frac{2i}{d}}}\n",
        "$$\n",
        "\n",
        "This means that each pair of sine and cosine components (for even and odd dimensions) corresponds to the same frequency-based angle. For example:\n",
        "\n",
        "- $PE_{(pos, 0)}$ and $PE_{(pos, 1)}$ use the same angle with sine and cosine\n",
        "- $PE_{(pos, 2)}$ and $PE_{(pos, 3)}$ use another angle at a different frequency\n",
        "\n",
        "The following table illustrates this pairing structure:\n",
        "\n",
        "| Position | $k=0$          | $k=1$          | $k=2$          | $k=3$          | ... |\n",
        "|----------|----------------|----------------|----------------|----------------|------|\n",
        "| PE(pos)  | $\\sin(\\theta)$ | $\\cos(\\theta)$ | $\\sin(\\theta)$ | $\\cos(\\theta)$ | ... |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPzwMVfcQpT-"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, k, d):\n",
        "    \"\"\"\n",
        "    Get the angles for the positional encoding\n",
        "    Arguments:\n",
        "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
        "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
        "        d(integer) -- Encoding size\n",
        "    Returns:\n",
        "        angles -- (pos, d) numpy array\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(SEED)\n",
        "    # Get i from dimension span k\n",
        "    i = k // 2\n",
        "    # Calculate the angles using pos, i and d\n",
        "    angles = pos/(10000**(2*i/d))\n",
        "\n",
        "    return angles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxp0Q5p8_4IE",
        "outputId": "6a1e8898-9f06-42af-8e7b-3ab87b0afa78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92mAll tests passed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00],\n",
              "       [1.e+00, 1.e+00, 1.e-01, 1.e-01, 1.e-02, 1.e-02, 1.e-03, 1.e-03],\n",
              "       [2.e+00, 2.e+00, 2.e-01, 2.e-01, 2.e-02, 2.e-02, 2.e-03, 2.e-03],\n",
              "       [3.e+00, 3.e+00, 3.e-01, 3.e-01, 3.e-02, 3.e-02, 3.e-03, 3.e-03]])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_angles_test(get_angles)\n",
        "\n",
        "# Example\n",
        "position = 4\n",
        "d_model = 8\n",
        "pos_m = np.arange(position)[:, np.newaxis]\n",
        "dims = np.arange(d_model)[np.newaxis, :]\n",
        "get_angles(pos_m, dims, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5xDUhmU_4IE"
      },
      "source": [
        "---\n",
        "\n",
        "## Calculating Positional Encodings\n",
        "\n",
        "Using the angle matrix I computed, I can now apply the sine function to even-numbered dimensions and the cosine function to odd-numbered ones to build the full positional encoding matrix.\n",
        "\n",
        "This matrix is added to the input embeddings before feeding into the Transformer layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "y78txxoHQtwG"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(positions, d):\n",
        "    \"\"\"\n",
        "    Precomputes a matrix with all the positional encodings\n",
        "    Arguments:\n",
        "        positions (int) -- Maximum number of positions to be encoded\n",
        "        d (int) -- Encoding size\n",
        "    Returns:\n",
        "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
        "    \"\"\"\n",
        "    # initialize a matrix angle_rads of all the angles\n",
        "    tf.random.set_seed(SEED)\n",
        "    angle_rads = get_angles(np.arange(positions)[:, np.newaxis], np.arange(d), d)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:,0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:,1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYiWrawRQvuv",
        "outputId": "a5b3a66d-665a-4691-b80f-a9e9fd86f80b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92mAll tests passed\n"
          ]
        }
      ],
      "source": [
        "# UNIT TEST\n",
        "positional_encoding_test(positional_encoding, get_angles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "rHkpvE5x_4IF",
        "outputId": "835f9ace-5068-4f09-ad4d-997fa4ef1b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAG2CAYAAAC3VWZSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAACw7UlEQVR4nOydB3hT1fvHv9lJkzbdkw723sgWWbIUwT1/ICJOVMSJe+NAwIFbVP4OFCcge4tMWTLKKKuF7t2k2en/eU9yQ4otowVKyft5nvPck3PPPXekTU7OOd/vK6uoqKgAwzAMwzBMPUFe1xfAMAzDMAxzNnDnhWEYhmGYegV3XhiGYRiGqVdw54VhGIZhmHoFd14YhmEYhqlXcOeFYRiGYZh6BXdeGIZhGIapV3DnhWEYhmGYegV3XhiGYRiGqVdw54VhGIZhmHpFnXZeXnrpJchkskqpRYsWvv1WqxUPPvggIiIiYDAYcP311yMnJ6cuL5lhGIZh6iVr1qzB8OHDER8fL75vf//999Mes2rVKnTq1AkajQZNmjTB119//Z86M2bMQEpKCrRaLbp164ZNmzbhkh95ad26NbKysnxp7dq1vn2PPvoo5s2bhzlz5mD16tXIzMzEddddV6fXyzAMwzD1EbPZjPbt24vOxplw+PBhXHXVVejXrx+2b9+OCRMm4O6778bixYt9dX788UdMnDgRL774IrZu3SraHzx4MHJzc8/jnQCyugzMSCMv1POjh3IyJSUliIqKwvfff48bbrhBlO3duxctW7bE+vXr0b179zq4YoZhGIap/8hkMvz2228YOXJktXWeeuop/Pnnn9i1a5ev7JZbbkFxcTEWLVokXtNIy2WXXYYPP/xQvHa73UhMTMRDDz2Ep59++rxdvxJ1zIEDB8QQFg039ejRA5MnT0ZSUhK2bNkCh8OBgQMH+urSlBLtO1XnxWaziSRBD7KwsFBMPdGbxTAMwzDVQb/ny8rKxPeSXH7+JidoWYTdbj8n1ys76buNpngo1Rb6rvX/DiZoVIVGYAi6fvqunjRpkm8/PTM6ho49n9Rp54V6bDR/1rx5czFl9PLLL+Pyyy8Xvbzs7Gyo1WqEhoZWOiYmJkbsqw7q/FA7DMMwDFNTMjIy0KBBg/PWcdEFhwNOS63bMhgMMJlMlcpoCodmNmoLfdfSd64/9Lq0tBQWiwVFRUVwuVxV1qGZkkt2zcvQoUNx4403ol27dqI3t2DBAjEc9dNPP9W4TeoB0pSTlNLT00X54U3LcTcSoWx7O3IzDovtGCTieFa22Ko7jkHh5oWYtnwnvolsjoFTl+DnxNb4eNVuPK1pKOpn5+TA2O8prBzYB03v/wGF6/9A3uKv8IgyGb81bS/q7Dx8HPd+uw5ZM19AyBWPi30fhDbB7sfvEO3fK0vCp2FNoel8NwrW/opHf9qI95fvFHXzFn4pzj14+lJxTd9GNcfm/w1D9o/v4Al1iji2w1O/YH6HLshbOku0sWLXEXR5/o8TbRzag73P3oWo4W+K4+mYVbuPim3R9pX495FbsOm2IfghuoW4tsPHssS56L67vjhX3EfMte9C2+VeFKz6SdxTemY2fti4H3E3foAbPlmBFuN/xMOzNyDiqtdFPXqGVK9o6zIc/2ySeEb/jL4a89p2RudJv+GjsKa49fPV4lnSdWzYl4Gs7BzxfhTuXoeJqhS8rm+Ew28/JO6f7m9Jzx5Yd/1A/BTfCtONTZD5xbN4TtsI4xXJKNy5VhxL152Wnim2G/dn4OfNB8Q90b3Qc33T0FhcLz1vaqfj07+Ka1o+4HKkjJmFjTcPFvdE95H26r0IHTAJhl6PiOet63a/eMb0nqnajxLXmZ/2r+fv59hR3z3T+Wk//f3QNdB7Qs/k960Hoe8xHrPW7RX3Tc/k7aU78NwfW8T56PlFj3gbsddPx02frUTCrZ8i8X8z0X/KYjQc+3/o9caf6PbSPDR/YDY6P/u7eN/bPvazSC0f/kmU0T6q0/O1+Wh097fo89ZCcTy1Q39D1Gb8TR+J9uk89L7e9c1acV76G5Xew8d/3oRJv/2DsMEv4sV5WxE28DnxLOh66brp70r6+wq+fCI+Xb1HPKcv16biq7/3+u6T/kbouf246YDYUqLnQM+D0p/bDontwu2HseTfw+JZLdt5WPwNU57+Tuk5Uvp7b7ovT38vUn7zgQzxvGn7T5onvzXtmEgif9CzpbT90Ik8/V9K/5+7vHna7vHmpS2l1COZvvy+oyfy+9MzfVspT+9/mjd/MCPLV9c/T/9j0ra6/Ml1KR09/t88ben/Ufq/9M9Ldf3zx7x/p7StLi/VPX6aPG2ry0t16f/65Dxtq8uf6rizbYM+n0/O07a6fHXHKVrdJL4zgoODcb4QIy5OC5StbvKdu0ap1U2i40IdLf/vPf+RkEuVOp828odGWZo1a4a0tDRceeWV4g2mzoz/6AupjWJjY6tto7rhshCDHmqZHDKFGiEhIWKrhlzkaSvKDXroXMEIkiug0unFVmcIhsb/OKUGBqUSCk2QqO9yWcV+vUIh6gQHh0ATZECIUguZ0iX26WQKBGvUvmvQyT116TXV1Rn0ot0QfZDv3HRNlDeoVQgJ0ol21DJAodFDr1CKutSGITgESq1eXKdoIzgYdq0acpVWHE/nozq0DQk2iOtQqlViH7UZ7L1/nT5YtEP3IVfpIFPS9QX57jvIUCHKVToDFBqzuG46h6jnfZ4hwXoogzz3TddNbSno2mQKqMV9BvuuJyRE7bsmug4tPSOdRlwX3Z9aqYRB5blOOj4kSAutTA6NrEIcQ8e6KuC7fmozyKXyPG99MDQWT5t0vfS8qR26FromvVIJuToIBqcKcpcOcpUdwVoNZN73jJ639H4o6L0R92aAWxPsex4OhcbzfnvvXUbvVfCJvN77fgQZguG2u0Serssuc4jn5nl+OsiVaqh1BsjVnrxSpxfXRu8F5G4oNBVQanWAwwUo3OJvuULuhlKrgUIjF/UUShnkarvIK5VycTz9DVGbFS6Xp32VDhVul3gfKE/n11Bd77WovXmt3gCZSgu4XeJ66bpF3ns/cLt9ebo3OakEvXmVQu7Li7qA9zmoPflgT562CjnEs9IHh0BBbXj/lmlLnC4vtjL48oT0/yfVrSof7HccvXf0683/fRR1T5Onre98J+XP9LiQk/JS3dPlxdb7vM6mjZBzdO6atFH/zq3y1LsAywzof006d02okCt81y3d37mEvmtPVvjSazqXTqeDgj7jFYoq65zqe/qSUBv5Qz3IgwcPIi4uDp07d4ZKpcLy5ct9+/ft2ydGUmhtDMMwDMPUZ+iHTm3T+YS+a/2/g4mlS5f6voNpaQd9V/vXoXWm9Pp8f0/X6cjL448/LjTnycnJQgZN83TUi7v11lthNBoxduxYIcEKDw8XPT1avUwPhJVGDMMwTH2n1h2QCsVZDxDQzIa/FJrUvvQdS2IYmm46fvw4Zs2aJfbfd999QkX05JNP4q677sKKFSvEsg5SIEnQd/To0aPRpUsXdO3aFdOnTxeS7DFjxuCS7bwcO3ZMdFQKCgqELLp3797YsGGDyBPTpk0TK5fJnI4URLQu5qOPPqrLS2YYhmGYesk///wjPFv8Ox4EdT5IPEPCGWmdKNGwYUPRUSHPtffee08sYP7iiy/Ed7HEzTffjLy8PLzwwgtigW+HDh2EjPrkRbyXVOdl9uzZp9xP8mky0zlTQx2GYRiGqS/IZLUceXGf3bF9+/YV0urqqMo9l47Ztm3bKdsdP368SAG7YJdhGIZhAgUZLXJX1GbaSI5AJWDuvPcn6bijbzJ63DEKbSatFtu7hzbGhsv64Ka20Yhq0R03rVPjf+k/YmWeGXOHh2HQqm/w1Bu/4t4XBiO553AM/XgjrCX5+G7JIcx69HKsvPlpLI+4QrR/xRdPIanH1bjz/7bircGNsPLxH9DhqithcVXgynt7YPnH67BR2woKGXD52K5C+bE/pjt+XrAPd7SJRHlBJlI/+AZ/F5TjyYHNEKtVou0NrbFj7n7I+96BPJsLbUI0OLZrL1re1AXODlfB2KAZvt1yDBm7j2BQkwjRhvufBTiydDdimjbFzhIb7O4KtI5QIUajhGXjIhz/+xAS+nTAnlIbShxu7MmzwKCUY/W+XORmlCC6bTTKC47DaTWhIqmtUMCkl9qxJaMYxphoHD5WitL8IrRNMMJalCPqaco8vjv2Q7tRvD8DurBYFB8uQnGWCaGRQeLam8YYkGjUiusJ1ymgNOdDLZehIv849Ao5wtUKmI/nI9ioQVCkDuX5FgRFh6DE6oTJ6YYiIg5ml1sc7w4KE0ojwuRwi2dabHWgyOIQbRaV21FgtsOglKHARFs51AYV7BYnNCEakRzWcqhDdHDZLXDZLFDpdXA77eJ9kQcFw+2wQ67VQ0aKHbcLFUotKtRB4pwVSg3s3guQtp682zOHrVCg3OESeZvTDavTU26xu0SSe8upjJ6v3ZtXKJVwUl4mg8vphsvlhlwpF9sKdwUUSjncrooTeTKnkgNyhdxjVCWXQUHPlFRFSs+/NuUVcr+8zLNf4VfXl3d58uJ+vXUl5KLco3ai5y0h5Ul1RMm/jNrzx7efrtlX1+84v0+jqoQedE3V7a+qvqwefnDKT3PRZ6KAOV0bDHMpwCMvDMMwDFMH0A+Z2kwbVZxntdHFDHdeGIZhGKY+qo3kgdt5CZhpI4ZhGIZhLg145IVhGIZh6gAeeak53HlhGIZhmDpAJpeLVGPkgTt5Erh3zjAMwzBMvSRgOi8H/1oI9Td/YHl/K479swwrrlIgfNbv+C01D903rsHid67D/BlfYNrozzHu2uZY1uc2jF3nRnF6KvLvfAPfP9UXG2f/hK433YhItRKt/p4hjp0wYz1uHd4Ua6L74fmxl2Hr3EXIf2sCFhwvxae3d8TQxBDETXwZGwoteGrOvxicHIoGjzyD6Na98Mz8PTi+dSVcf0yDPioR65YcFtLq7kHF6JViRPLoO7C5yIqlGVYYVXJ06pEgrifqutux7HAxElo1x+rNGSg6ugsJ5sNi+PH4whXYvysPHdvFIsfmhE4hg/LA32gVosaxlVtxaG8+9N36i33E30cLhYx6/8FClGQeRmyXJkIOThSpI6DWG7E9qwxbjxYhLMaAwqwymHPT0SrKALu5RNSTZR+AQq2D9chBFKdlQhcWjdJjpV6JdDBKnS40jtQjIcQTrC9MDSjKcoWE2ZmTLu5NSKWzCxAUoYMhWo/yQguCYiOEnJuk0sqIWCGTdlV4pNISJjtJpWUotDhRaHVAK5cjt9SGApMNOoUcVosDWq1SyKPtNifUepVIJI9WB+vhtFuERFoVEgS30wEXSaT1IR7JtD4YbrVOnMdNgQ29wQYdFRDXQs9b2lIS8mgKbClX+OTRVhdJpV2Qq9RCJi3Joi0Ol5BJU32STYu8XCZk0SSDrnCTNLjCK1GuEOW03+32SKVJMusmCTkFVPTWIZkzSaTp2pVe6bMkhZZkxrTf7XaJoIxSXUkqTUhbgiTM0nGSbFo8F2+dSvLok447ka9e7nwm5eLvqxrpr9wrhvbfLz8LibW8Ggny6aTGJ39onk0AP1Yxn1suQOzEgI5tdDHD00YMwzAMU2fTRrVZ8yJHoMKdF4ZhGIapj+EBZIE78hK43TaGYRiGYeolPPLCMAzDMHWBwhNOpKZUnGVgxksJ7rwwDMMwTB1Q20W3sgBesMvTRgzDMAzD1CsCpvPy1pSJGDBmCt7oOR5T33sCU7uMxeUP/YAnnrgClz27FPLnRiGu40A4KiqQ+PVv+P1QEeZ89H/odsstuO61FWi66B0ERcRj/v3dMPqJvpj94LdoZtDg8Nq5aD3jI0z4dCPuCM+DrawQ8977S0h/k7fORp/JN+KbwxWI1yqxe/lf6PXiCMwrDMGVQ9pg09JtIqrxlukL0KhbD+wqtaJlsAbFP3yI9mN7o6jZACET/mBlGrqH69Dyf/2ErDczvA1mrjuCAV0TkZm6Fw5zCSxrfhPXd3jZQew32TGiXZyIvJwSpEbx6sVI7BSLjL8zcNDsgDO5s5Bk0zWuTM1FE4MK+ceLYSnIhLFDB3EOkj6nFVlFm/8cLUJmRgmaJ4eiLDcLdlMREkNUoh71/B1H9wpJddH+DJQcLUVwuA6FBRaPVDrWIO4hJVSHSJ1noE9Rmg13brqIJm3POiauIyhch7IsEwwxegTFhaPQ7oJeSKVdsJJkOCzGK5UGLBUKX/RiKZJ0QbkdhSa7kIYXmu0o9kaTtlmcUOnVUOlVcNic0Bg1UIcEiecu5NEOu5BIK/TBcElRpfXBnsjKaj0qVJ5I0lBp4fZKpa3OikpRpaVfTySPJpk0yZ498meV2JJEWooqXU5Rpf0iSVPeE2laBoVCLqJJi63LG1Va4YkwTdJpKid5tEdO7ZFNk0xXklCT9Lmq6NCSJFqSUItn5yeP9h13kqzaFx3a7T5J/lw5erRU5v9aiijtL7GWokZ76p6QW/uX+9o7SVQsXZNU5eT9p4/G7Fe3UrnsvEhxz3Vk57ORY59Vu+elVeZMYal0zeFpI4ZhGIapA8SPHQ4PUCMCZuSFYRiGYZhLAx55YRiGYZh6aFInY5M6hmEYhmEuJKw2qjmB221jGIZhGKZewiMvDMMwDFMH8MhLzeHOC8MwDMPUAdx5qTkBM23U/7dXYYhpiFitEsMXvi7KijNS8e/ot3Bg5W/4YOZ2rHhzGB779A70f30VRvVJgsYQhsX3X4aj6+bh84k/Y+JjNyJzwu1QTpyOzUVW3PrCEIQ0aIaXt9lxYOUf2PbgE2jYexh2lFgxon8K1j72OczDHsW7P+zAsH7JMOUcAW54Cm/8shPPDWyCgrStiGjSCav+zcX9w1sKv5JevRtgxxdrEX7LPfhhVw4SdSrs/eco2lzbEuqBo6CPSsTv+/Kwe1sWbuuUgNJj+4Uny8E/1iOyaQf8m14iPFJ6JxmFf0qLWD0yVuxEYt9WOHC8DHk2J9JKK8S5qO3jh4oQ3zYaZZlpsJtLoGzVXfxDaI2R2JpVCkNMInYcKURhjgkdk0NhKcoW9UIrzOIZKnUGlO0/CI0xEsVpOSg9VorQKD2yrS6UOl1oHK4XnjJxBjWC3eXivOTz4sw6LK6vLD0HRq0S+pggmHPMCIo2QB8bjhKHG6qoGOHxYiHPk6Aw4fNClNrdYktt5ZfbfT4vuaU26BRyFJhswt9FE6yGzeKAJkQNbYgGDqsV6mAt1MF6OO0WqIM9fi/CryYoRHiTkJ+ITBcs2q9Q61Ch8ni7uFU64dkiebtI/i6S3wtZfJc7PN4ttIjO5vR6uzhcPp8XOt7u8tSxO12Qq9Qi7yRPGIUccqUcbtGuTPi5UCL/l4qKCuH1IsorKlBB/i7ecoVSLnxahDePXCZ8XDzeLgo/vxZPmdvPx0XyeZE8YSTPF2kr/G5EXc89k4eLv/+L2C88WshjxiXKfP4v1ZicnPCHkfnVrezvcjKe81S9r6rD/IuoXemaz9eH5dn4r8gukCfMheZU79/5og5OeV4DM9Y4ybjzwjAMwzAMUy/gaSOGYRiGqQNktQzMKKvFsfUd7rwwDMMwTB3APi81J3DvnGEYhmGYegmPvDAMwzBMHcBqo5rDnReGYRiGqQO481JzAmbaaPqH67H7s9swZu9CvPbKUkzc9BneeudhjJ7wMfqNG4thCSEwP3Qzfm9zN1IX/4xOCxfixefvwO6bR6Jx35HItDrwdFwmZn69Hdd/vBHXt4iA9a7Xccudw/DFF0ugC4vBnGWH8da4rrgsTIuOU1/Cn/vy8di8VBxZtxgdXn8CxqSWeGPlYRz4aw0SUv8UMuMO/doj0+rEHW0i0ScyCK3vG4G1h4qwS5aAb5eloWfzcOTv34zkO27CdkswYlt3xg+rDyNv3xa00pULiayxQTMcWpOORq2jkWFxCOlweMFeNDOokdSrATI2ZiLs8r44Um4HKXvXphchUq1Ao+ggFB3PRFyXFFiKcoSs1BLeSFyXPioJ6w7kIyI2GPnHy2DOS0fb6GDYyopEPWX+IciVaqiDQlC0PwNBEQkoPlqCnDI7UmKCUUQSYVcFkkO14vlH6BRQlGbBoJTDcewgyo9lCqm06Xge9DF6GGL0KM+3QB8XgaDYCJicbigiYsWWJNLuoDDfe2myu8U9eqTSDmjlMuSV2lBotsGglKHMbIfN6oBar4KdJNNGDTQhGrhsFqhD9NCEGuB22EXe7XQIqa88OFTcl0gkkaatUoMKVZA4pxNyP3m025cvd3hk0BQZVpJHU7J6JdEkk6akUKq9smmnkEiXe+XTJHV2udxiS5JTkkRLZW53hZBPu5weebRCIRdbt1/eJ3l2u6DxyqYlKbR0P/7SZX9JtH/ed5yfvJjy/rJpKe/flr9MVpJe+0t/TyVXPrncX/5anRRWXoXguCqpcXXnlFcjcz5bufK5kEifS876+s/JOS8RvTJTL+GRF4ZhGIapA+hHgfTDoGYNyBCocOeFYRiGYeoAMpukVJvjA5WAmTZiGIZhGObSgEdeGIZhGKYOoLVTZ7N+qqrjAxUeeWEYhmGYOoCmfeS1SLIaThvNmDEDKSkp0Gq16NatGzZt2lRt3b59+/o6Wf7pqquu8tW58847/7N/yJAhOJ/wyAvDMAzD1AHii15+YUdefvzxR0ycOBGffPKJ6LhMnz4dgwcPxr59+xAdHf2f+r/++ivsdrvvdUFBAdq3b48bb7yxUj3qrHz11Ve+1xqNJ6jt+SJgRl7uv601Vje5DB3f2Y0xAxtiwGIZbt3yERQaHRYOBAZtnY8Zs/dgwnPfoNmA63DFlHW4z7YWM/88gN8n9cVdN7bEosEPC3nu1t9+Qf85b+PmTzZiav9oFB7agZ43DBOS3qvcuzHsiQFYJmsu6i797W9x/m0R3dD+yh6Y/ccelBdk4t/JnyPpsn549apWIrqz649p6HBHJ2DQPci2OvHuyjQc2boDbcf0hsNcgvL2V+PzDUfR7bIGOLxtn2jDtfZn6MJikdCqOXaU2HBbtyQhT47XKlH+11w0bRmJxIFdsLPEBrToJSI1k1R5ye5sNDGokXBZnIh0HdWjI5xWk5A+HyyyISgiHqFxMTh4pBgpSUaU5OTDWpSDxmFaUY+wp/0Ltd4ozl+YVoDgcAOKs0zi2lsnhPgkzjFBnv6xxpyHitx06BVyOLKOoCwjF8ZwLUxZZTBEB0EfG4oSqxOGhCgooxJgdrnFltogebddqRPtUPTignKH2AqptMkTSTq3zIYCk13cH0WUFhLpEA3sNs9WHaITEaRJJu2RSNuh0Bs80ZYddsiDgn3SYrdXHl2hDoJb5ZF627yRpMW9uynv9kWKFvFJpLzX7pvk0SSJpq2n3FNm9+Zpq1AqheRZihgtZNGiXZL6VogI0x6pcYWIJk3lVJeiSYsI0+4KIXEWMmdfdGjPv7S/VJok1CQH95dQ+0usRd77ISjJXymatL8Utqq8iCrtLa4kma4knz7xP+h/Dqmc8lL1k2XQkty5yujRIoq13+tz+EFX05H4s/0OOl39M/liqsn3Xn2faAjgmZJzwtSpUzFu3DiMGTMGrVq1Ep2YoKAgzJw5s8r64eHhiI2N9aWlS5eK+id3Xqiz4l8vLOyEvcX5IGA6LwzDMAxzMaqNapOI0tLSSslms6EqaARly5YtGDhwoK9MLpeL1+vXr8eZ8OWXX+KWW26BXq+vVL5q1SoxctO8eXPcf//9YoTmfMKdF4ZhGIapA2jksbaJSExMhNFo9KXJkyejKvLz8+FyuRATE1OpnF5nZ2fjdNDamF27duHuu+/+z5TRrFmzsHz5crz11ltYvXo1hg4dKs51vuA1LwzDMAxTj8nIyEBISMh5X29Coy5t27ZF165dK5XTSIwE7W/Xrh0aN24sRmMGDBhwXq6FR14YhmEYph5PG4WEhFRK1XVeIiMjoVAokJOTU6mcXtM6lVNhNpsxe/ZsjB079rT31ahRI3GutLQ0nC+488IwDMMw9bjzcqao1Wp07txZTO9IuN1u8bpHjx6nPHbOnDliLc0dd9xx2vMcO3ZMrHmJi4vD+YI7LwzDMAwTIEycOBGff/45vvnmG6SmporFtTSqQuojYtSoUZg0aVKVU0YjR45EREREpXKTyYQnnngCGzZswJEjR0RHaMSIEWjSpImQYJ8veM0LwzAMw9TDwIwVNTj25ptvRl5eHl544QWxSLdDhw5YtGiRbxFvenq6UCD5Qx4wa9euxZIlS/7THk1D/fvvv6IzVFxcjPj4eAwaNAivvvrqefV6CZiRl73jp2NDoQVpq+fDMOsPrJv1DV6b8DPmfjAOX3Qbi75fpuH6FhFwlJdi2fP9sO23H/DtDW+gvVEL/YzH0OSrXzHvWCnuGt8TmuAwfGZqjO1//Iq0R8YhsdtV+L/bO+DaznHYcN9LME54F0/P2oKrO8cJD5ikrldi4o/b8d717ZC1bRlCU9pg2ep03DWyFTra96Nvp1hsmb4Ayfc/hB925SJWq8S6tUdRemw/Qq+/W3ip/Lo3H3+tT8fY7skoOrJLeLIc+W0Zwpt0Qu9OCcJfZWCjcBhVcrQN1+HIwk1IHtACQT2GIcfmxFGHXnhykKfMwbQCJDePQFyPVrCW5EHdro94RlpjJDYeK0FIXCNEJgSjILsM3RpHwJyXDru5BFFKj1GRUmuAaf8+aIyR0EfGouRoKUKjgnDc4kSp040mEXpYXG5R1yh3CD8WRUmm8HcJVytQlp4D07F86KP1KMsywRAfLPxdCu0uaGJjoYhKEMe79RHCU4Uosbp8XiH55Xbh7aKVy5FbahPeLgUmG8rNduj0atgtDtgsDuHv4rDZxVYTGgyn3QJ1SBBUwUFwOx2QB4fB5bALPxG5PsTnK1Kh9njKVKh0J7xdXBWwushfRSHy5Q6X8HehrUKphlypEnl6XyR/F6pbbnfB4i23O13itSfvhlxBnjAy4edCfi/0IUaeLsL7he7fXeHzfnELXxi58Hghrxfh7eL1aFH6+bVI5WqFZ0v+Lv6eL5InDG2pXDxT+gAVfjIu8TdCHi+eZ33Ca0U8D2+e6ot2hdeKfxuV/+fEc63CH4Y8XvzLq6K6z+QqPV8qHee5j+rqesr9r+nsPyBP9l+pyXdPfY+nd7r373xwKfq7kHdTbVNNGD9+PI4ePSqmgTZu3CjM6iRoke3XX39dqT7Jn+mz58orr/xPWzqdDosXL0Zubq6QYtPoy2efffYfRdO5JmA6LwzDMAzDXBrwtBHDMAzD1AEcmLHmcOeFYRiGYeoAWlpSuzUvCFi488IwDMMwdUBN5M4nHx+oBHC/jWEYhmGY+giPvDAMwzBMXa15qc3Ii4xHXi557n30Pby06m0888aj6DP2A/S4YxT6RAYh4o1xOFLuwOYfv8UVGxbj8WfvQt59NyKpx9XYUWLDqB8m4tO3VmDoxxsxItkI3fOfYORd1+HVaQuh1hvxw0978Pr93WH/8An0/OQF/LzhGCbM24t9Kxai+/QnENKgGR64pR12LV2FZukrhEy23YCu4pwPdm2AQ9OmoOOjI7Dq31ykBrXAZ4v2o0/TcGTvWiuktntk8Yht3RUzlx9E9u4t6BbugstuEe0eWHQIjdvF4tZOCUK2GltyAM0MajTsl4yjqzMQPXAASqNbCbnx6iOFiNIo0TwqCHmHjyGhVyOEdLtcyEot0c2h0huhj0rC2gN5CI8LRqdGESjNOoIOcSGwluTD7bRDlZcmrp+k4kWpRxEUkYDQaD2yi61IiQtGvt0Fk5BKB4EUxnRNipLj0ClkcBw7iPIjRxClUcCUniMk0sHxBphzzEImrU+IQonDLWTSstAYcc0u/QkzpFK7W7RHbRWU26GVy4REOq/MCoNShhKTHdZykkerYbM44bA5oQvTwmkxCZk0SaRdNovIC4m00w65PtgnISZZtE8qrQoSW4dMCZvTLd4Hq9MtJNIi73Cj3OGGnKTQJJmWKzzlLk9dSiSVJgk1yaQtdqeQT5NM2u6VPDtJPk0ScpJFe8vkCpnIC3m0s8IjjxaS5wohm5bykiyathqlXMieCbVS4SuXZNBEVXna+urKZOLZEtL8u5A5++Wl/aKO3wemaMNFkmiPjF3a7y+x9ufkcjpE7hU7S82eXEfaX6nspKJTnbOmMulTHXvaujj/nPX1n5NzBu6X5cUcmDEQCZjOC8MwDMMwlwY8bcQwDMMwdUEtF+wigBfscueFYRiGYeoAVhvVHJ42YhiGYRimXsEjLwzDMAxTDwMzygN45IU7LwzDMAxTB3B4gJrD00YMwzAMw9QrAqbzEte2F65cF4EHUj8TnhsrhgJX7VqMaZ9vxdOf3I7mV16PXtO34inZOnzy4x4sfOlK3HNLK/wYNVT4V2yc/RMGLXgPIz9Yj8+GxCJ//2b0uWW48DW5Abvxy5vLsFTXEWq5DHN/XC08J7bHXYHOQy/H/a0NMOdlYMfLH6Bhjysx5do2SNSp4P71baz9YScwbDwyrU68uXw/0jb+g/b3XAGHuQThjdrjvTWH0LtXMg5t3SvacK2ZDV1YLBLbtMLWYiv+1yMZHcMg2jOv/BUt20YjeWg37Ci2Am37458ss/BDmf9vFloGq9GgezxMOUcQ06szKpp0Fb4taUU2BEXEIywhHvsPFaFpwzB0TQmDtSgHzcJ1cFpN4hnaD2wX3jZ0/vx9eQiJNCIqxoBsqxPtEkPFsyB/lpggz4CeTiEHco4gRKmA4/hBlGXkwhiuRemxEpiyTNDHhqLQ4hA+L8qoBJhdbrElfxfyibEqdKId8hgpKHeIZ0spp8wm2iZ/l9xSG4wqhfB4sVuc0IRoYLM44LCWQx2iE544mlCD8Hdx2a1Q6A2QG0LhdtjFVvJ5cWsMvr8Vt0ortjZXBax0IXTv7grYvT4u5O0i+bt4fGDkPm8X+tsifxfydJHKyNuF8rS1k/+L19uF/FyEp4toF8Lrxe2q8HqWVMBdQb4yEHUpHD0tziO/F7pe8nahe/D4tch93i5q4QXj8X9xe+9N8nMhPxZ/HxiqSwi/CFHuhoqCrXjx95CQ8tSmzxPG3+/F6znhafvE/53k/SLy3nKqJxWf7OEiebVU94PSf5Rcdg4/3Gr6A/ZsR+1PV/9MfknXZKagvv8+v1QHGOj/u7YpUOFpI4ZhGIapA3jNS83hzgvDMAzD1AEsla45F82g05tvvimGTCdMmOArs1qtePDBBxEREQGDwYDrr78eOTk5dXqdDMMwDMPULRdF52Xz5s349NNP0a5du0rljz76KObNm4c5c+Zg9erVyMzMxHXXXVdn18kwDMMw51ptVJsUqNR558VkMuH222/H559/jrCwMF95SUkJvvzyS0ydOhX9+/dH586d8dVXX2HdunXYsGFDnV4zwzAMw5yrNS+1SYFKnXdeaFroqquuwsCBAyuVb9myBQ6Ho1J5ixYtkJSUhPXr11fbns1mQ2lpaaXEMAzDMMylQ512XmbPno2tW7di8uTJ/9mXnZ0NtVqN0NDQSuUxMTFiX3VQW0aj0ZcSExNF+cZnu2D9t7Pw4oRfsPbzezG16z3oNm0nbu+egFktxmDjywOw4/fZ+OS6N9EnMgiyV8Yi7pM5eOqNX3HvC4OFlPjNrHhs/+Nn7L7rTjTqMwI/jeqIW/qlYM3/nsOuUhue+GIzru+fgsJDO9Co1xA8NGsLZtzYDnnvPY+IJp2wYOVRjL+pLdoWbcHAXg2w6a352FxkwcztWULq/Neqgyg9th8hNz4AfVQiGndpgbV/H8UDvRqi6MguKNQ6pP2wCFEtumBQ9yQhUR7aJBz4Zz46xBpwaP5GNBzSFtqew5FpdSDNqsWCPTlICVLh4P4CJLWJQoMr2sNakgd1x3447tJDFxaD9RnFMCY0RkySEfmZpejZNBIdYoNhKytEtLxcPD+l1oDSXbuhDYtBcHQcig8VIzxGjzYJRhQ5XGgeZYDF5RZ1jTKbkDSTRNuRsR9RGgVKD2ehLD0P+mg9yrJMKCm0IjgpBoV2NzSxsVDGJonjXYYouPURop0Sm0c2S23lmj3yaL1Cjqxiq2hbSKTNDqiDVLCa7UIirQvTwm6xwGkxCXm0w0pbA9TGYLiddsiDwyDXBwtJriSVJirUHlk2QfJnwuokeXSFVxJdgTKbCzLFCam0XKnybtVCIm2yOkVdem3xltudLiGbprzN7oJbyKJlQhJNkmmFUga30+3J+0moST4typVyIZOmcpJHS9JupVfyrFYqfOUkfSYJtNtfHu2VVZNMWpJIE7T1SZtlJyTNlJeeSaW8t11/ibSnDVQphZaQ8uJ5VzPETcWe/VX/T1d1mH+Rf7vVjaL7D6+f7sfqmXwo1uQHb33/kVzd+3c+udRnRcTUj7wWSXaJP6CLsfOSkZGBRx55BN999x20Wo+vxrlg0qRJYspJSnQehmEYhrnY8Pg01S4FKnXWeaFpodzcXHTq1AlKpVIkWpT7/vvvizyNsNjtdhQXF1c6jtRGsbGx1bar0WgQEhJSKTEMwzAMc+lQZz4vAwYMwM6dOyuVjRkzRqxreeqpp8R0j0qlwvLly4VEmti3bx/S09PRo0ePOrpqhmEYhjk3yGs5euIO4JGXOuu8BAcHo02bNpXK9Hq98HSRyseOHYuJEyciPDxcjKA89NBDouPSvXv3OrpqhmEYhjk31Hbqx82dl4uTadOmQS6Xi5EXUhENHjwYH330UV1fFsMwDMMwdchF1XlZtWpVpde0kHfGjBkiMQzDMMylBI+81GOflwvFdx2uxthnH8GNneJQcOPVomz3gjloumgJJj01A//07Y+2V98kJMY3rPoIMz7ZjIFvrkZxeioK75yMiY/diKnvzoEhNgWz5h/AzEd74/jjo9Dli+n4ZVcuBkbrkbZ6PrrMeFPIol+8szP2LFuKBpu/xYoP1qDf8G4icvTYFkHY8/q76DBpDJbtKxAy4C/mpaJvp1jk7FwjJMnrTMFo0KEr7h/YFFk716OdukhERw5LaYM9y4+gfZcE3NGpgTg2Mmc7jv2xAI0HNUba2mOIGjQMeaFNRFTmpQfzsW5nNlo0CEH+4cNIuqIFgnsOELLUktDG2JxZBkNMQyzbk4OoBkb0aBqJ0qxD6BJvRLJR7ZHRZu8TEm2tMRIFuw9DH5WEsBgDMkttaJ4YirYJRhFNukl4kDgnyWuVRRlCyhymUsB0ON0jlT6ShdKMMoQ0CIYp04RCuwv6+GiUOFxQxiQBxmgRuZlk0sV2j1S5zOb2RZLOL3dA65Vf55VZYVTJoVNTNGk7tEIe7RRSacqTTNrpjSZNkZc1YcGQB4fCRZGkg0OFXFrIiNW6E1JpVZDvb4UiSYso0C63kEj7R5KWnxRV2uzdCnk0RYwm2bTIO4V8mmTSnnKP/Nnl9EikSfpMZSR1pEjRUoRpiiZN+0VUab+8L5K00y5kz5I8Wvrwk/KShFoqI/w/HH3HyWS+6NAqhdwnm5ZMr/ylzSSRrhRBmtp2eaTNksT6VDJa6TokqKoUTVrkqzj05GjTVSFds9TOmXDyuU533NlIUS/E18jZflfJ6qlEOlBgtdElMvLCMAzDMIGCUk6p5h2QioAZfvgvAXzrDMMwDMPUR3jkhWEYhmHqAF7zUnO488IwDMMw9dDnxRXAnReeNmIYhmEYpl7BIy8MwzAMUwcoZBRMVV6r4wOVwL1zhmEYhglAqfSMGTOQkpIivNS6deuGTZs2VVv366+/9kS/9ksnB1OmyPcvvPAC4uLioNPpMHDgQBw4cADnk4DpvBTZ3XizZA6SFy3Bd3+lY+KmzzDw3rvR/bH50Ecl4vtNmVj3VA889sZwjN8ThpbBGuEDc9lNN+O6N1bg6bhMlBdk4sHxI4V/SYctX+HbL7fivXQDEnUqDPtwNNR6I2aXxuOm267A9SG5sJUVYt3TX+HvAgvevrol2hu1KPj4ZSydn4a8DtcJr5MB0QYc2bQOHR8dITw8olv3wltL9+O6AY1xbctIWEvyUD7/SwTHNUajTi2wtdiKu3umoJm8AM0MauTP/QlpC/cjcfgA7CixwtmqP9YcLUG4WoG5W44j61Aukvskw5RzBJGX94KzYVfhJbMztxwr9+chMikGhw4Wom2TCHRPCRf32DhMC3XOPuFfYtu9UdwX+bvk7y1AaJQeCfHByLa60C7RiBaReuHvEqv3DOKRD4srM008I8nfxRitR+mxUpRlmRCSFI18m1PcuyImCWaXG/LoJLiCY0Q7ZqhRYiMfEiDHbBMeLzqFHJnFFtE2pfxSG0JUCujCtLBZnJ6t1SH8XcjbhTxxRD4sGC67FQo/bxfKQxss8m61wff34VKe+GckbxexdVXA6nRX9nlRqlHuIP8Xt8/bhfxchP8L5ZVqT31vnvYLTxevtwt5vZCfi9gqZMLrxe3y+LnQ/DeVK5QyUdftckNJdZ3OE94tLpfX80Xh835RCy8YFzSUJ+2l18+FoPqS54vk70JblVwmzlfhdgsfD8nzxd87xf+DUdov+bt4ymS++gr5CT8QyftFKpeQ7ELIw6Uq65Dq7ET8P5+rquJ/nPwsPFpqal9ytt8X8nPgJRPASxuYc8yPP/4owu68+OKL2Lp1K9q3by/c6ylQcnVQeJ6srCxfOnr0aKX9b7/9tgiq/Mknn2Djxo0i1A+1abVaz9t9BEznhWEYhmECfeRl6tSpGDdunAiE3KpVK9HhCAoKwsyZM0/ZwY6NjfWlmJiYSqMu06dPx3PPPYcRI0agXbt2mDVrFjIzM/H777/jfMGdF4ZhGIYJgM6L3W7Hli1bxLSOBMUPpNfr16+v9jiTyYTk5GQkJiaKDsru3bt9+w4fPozs7OxKbRqNRjEddao2awt3XhiGYRimHlNaWlopUSDjqsjPz4fL5ao0ckLQa+qAVEXz5s3FqMwff/yBb7/9Fm63Gz179sSxY8fEfum4s2nzXMCdF4ZhGIapAzwxxmqXCBoRodEOKU2ePBnnih49emDUqFHo0KEDrrjiCvz666+IiorCp59+irqEpdIMwzAMUw9N6uTeYzMyMsSiWgmNRlNl/cjISCgUCuTk5FQqp9e0luVMUKlU6NixI9LS0sRr6Thqg9RG/m1Sh+d8wSMvDMMwDFOP17yEhIRUStV1XtRqNTp37ozly5f7ymgaiF7TCMuZQNNOO3fu9HVUGjZsKDow/m3S1BWpjs60zZoQMJ2XRzZ+g2fGfYse987Ec69fhQGLZZjbw4ScnWswb9oo3DW4Ef7ueSW2XP0MZk3/GnfOfwWJ3a7C0ge74vDauVg0+GF0u+UmvNjUhFEP98JP93yJEocL7360HLeM74kjl9+HjsOH4sUvNuOtwY2w68lJSOo+DAtS8xGrVSJi7UxceUc7rJm2EvtNdry+4iA6hWrR+aE+Qp6MYeMRltIG3Xo3xL9/pWLsZQ0gW/MdtMYo7J61Bg3adcT/+jZCicONfkkG2Fd8h3YtInDg963YdrwM8q7DkWdzYXNWOX7ffhwtg9VI35ePkvQ9SBjYXci25W37Yn+pG0GR8Vh9qADb9+ejUeNwFBzPx+VNItE2Wg+HuQShlhw4D2wVEunCf/chKCIexphI5B8uRnRCCDolh6HI4UKrKAMahKjF8zXYCqFTyBCiVMBxJFXIpCPCtCg9WghDvAGlGWUoLLUhOClGyKRLnW6o4lNgcVXAbYiETR0s2im2uVBQ7hAS6ewym2hTr5Ajq8QKo0ouUrnJDo1RDW2YFlazXWzt5WY4rSSVDobDYhKyc1VoqNjKDaEikXRXFmREhUYvziVtCZJEEyRztro88mgqM9mdkCkUMNldsDhckMnlQjJtsjmFRNpkdUKhVHtk0V4pNZWTRFqhVMLpdMMpyuVwOtxwk2xaKfNu5T4JNeVJOl3hroCcpM8VFXC7KyrJnEkK7ZM8e8vF0LFcBre3zFdeSVbtOY6Q5MziF59P2nziF5wkiZbqUrtSntrz1PG2Ic59oq5/uX8bvvxJQme6Jjrt2cimq2r3TKgkt5bV7IOwuh/INf3dfCYS6Rq1i/rNeXosjBeSSX/++ef45ptvkJqaivvvvx9ms1mojwiaIpo0aZJUHa+88gqWLFmCQ4cOCWn1HXfcIaTSd999t+/veMKECXjttdcwd+5c0bGhNuLj4zFy5EicL3jaiGEYhmHqAKVcJtKFjG108803Iy8vT5jK0YJamtpZtGiRb8Ftenq6UCBJFBUVCWk11Q0LCxMjN+vWrRMya4knn3xSdIDuueceFBcXo3fv3qLNk83sziXceWEYhmGYehhVWlHDY8ePHy9SVaxatarS62nTpol0Kmj0hUZoKF0oAmbaiGEYhmGYSwMeeWEYhmGYABp5uRTgzgvDMAzD1AHSgvvaHB+o8LQRwzAMwzD1Ch55YRiGYZh6bFIXiATMyEufr7NxW89E4XfyU5/HsG7WN3i/93g8+cpDCJ08Dg1/mo+fduZi9KTvoAuLwZvl7TH3xSux++aRaNRnBOYdK8Wi+7pi1cgHoH/2Y2wotODWgQ2Rt3cDIl/8BGM+24ivRndG+oYFyH9rAv6YdwCPj+oEV0UFhl2RhL8n/R8aPvU81uSXI1GnwoIFu3HFja0QOfYJ6KMSMXN7Fpr3bI8nBzRD/v7NiM9YhwNf/YLoVt2weVsOhl/RENe2iES4WgHZup+w/8c1aDqyI/7dlYcMiwNpzhDhjfLzjkzs3JWLJu2iUXh4DyxFOdB0HSx8S47JwrD6SCFCE5thxa5s5KYXo1/LaJRlpuGyhBDEKcrFs6o4tBUl27dDFxGPvB1HERLbABFxBqSXO4XHS9u4EFhcbjQM1SJcbhPnVRYehVGlEP4uJQePIypIhZAGwSg+WgJjUiiK88uFD402MVF4vNDxrpBY2N0VcBmiUGT1eIgUWVzIEv4ucmSbbMLjxaCUI6vYItoPCtF4vF1CtdCRv4vFAm2oDk6LCQ6rCdqIELjsFrjsVsiDw+By2KEwRkAeEi58RdwavUiEW33C58XirBDPiJLNmy+zuYSnC/m40Lbc6+NitjuFvwvVKbd7vV3I58Xu8X6hvI3KFeTt4oLbVSH8XMjbhV4LbxeXW2zJ/4Xy5PFCXi+inLZOdyWPFrfDDrVS4fN5UZMXjHe/5P9CeVHfVdnbRfpw9PjC0NYthptVXk8Y+gD05b3D0MI3xu9zsaoPWPJ4kepTe7683HOu6qBqUnP+o94n+8D46vvlhd+Mt+3Kx54ZJ4+yy2vpv1JdzdN9p5wvj5dzxdn66DD1J6r0pULAdF4YhmEYhrk04GkjhmEYhqkDWG1Uc7jzwjAMwzB1AE2x1q7zgoCFOy8MwzAMUwfwyEvNCeB+G8MwDMMw9REeeWEYhmGYOoBHXmpOwIy87F+5AKFz/sSSLybg6Ynvoscdo2BxVeDJwp8x/dN/0PvZJbjvuuYwZR/Buy/djncnfwPjJ49h5p8H8Mez/TAi2YgDd10v5NTXfboRIxuFodPMjxHbvh/u+H4Hdi6ch5il7yEoIh7z3vsLmVYn7kp2YljTcHR8dQIW7S/AQlO0kP0O7puEnJ1r0PTJJ7EwX4tG3Xrgi3mpeGxoC3RAhpCCHp05E1uWHEbv3inYb7JhdOcGCD/yNy4L0+LIT/Owa20GokfcKPa5KoC5qTlCgv339kzkHNiHRkPawZznaasgpBE0weFYm16CxTuzEZschuwjxShO34seDcKEnDolWAH50e1QqHUo37kFudvSEByTjPx9BYhMCEHblHDk251o38CI5pFB4pxxQXIoC47AqJLDfmg3ItUKxGqVQipNMmljgxCUZZsR0jBOyKRLHC6o4lJgcrqFRNodHC3emyKbG8VWl5BcZ5lsIpFE+lihRTwvar+0zAa9Tink0dZyh9hqw7RCIq2LMAqZtMtmgSYsGG6nwyeRdjvtkAeHCnm0RyodjAqvRNrh/fMX8miX2yeVFpJolUceXWZ3+crM3nKSSVvsHgm1yeb0SKWpvlc2XVnyXCFkz1KZkEV7y2krSagV3nyFu0J8IIlrddo9MmiXRx4tlUuyaJIz035JFu3/Qegvj1bJT8imVd5JcpLCSh4R8koyZ5loVyoXbbhIQl21XPlkTi73V9xK+ZM/byWJdCXps+zUbVcnk/aXIJ/uc702H34X4isjkL6XAlWZLfm81DTJA+mPJFA7LwzDMAzDXBrwtBHDMAzD1FVso1oMOykCdciKOy8MwzAMUzf4T9nW9PhAhaeNGIZhGIapV/DIC8MwDMPUAQox9VO74wMV7rwwDMMwTB0gr6ViSM5qo0ufV958BJePmQ75gzchufsgrBgKPL7gJbwyeiauaRKOw2vnIvzzX3DX43fjxkM/QCaX46M3V6C9UQv9jMcwaMF7mDknFd3Dddjy628Y+PNkvLTDjWfv74NVs+dDplBgyaM/oMd1g7GjxIqB0XoceuEJ9H5rNHbH9xHS4Jd/2oEhraPQ7tl7hfx2i7o53pq7B/cPb4kjm9ZhSKwbOd98hLCUNvj3x52inYcubyQWZTUyH0Dm7B/QYmhj7Ju3H7tKbShO7Crk3vFaJeZtzEC7eAOy9h8Vcu+IAUOE1FalN2LD8TKEJDTDwl1ZOLy/AD1bRaMw/aCQSDeP0Ih66sydsO7aAK0xEjn/7EXennyExwXjeG45miaFolNyKEocbrSONiDBoBK/FlQFh+A8vAthKgXMaQeETDokIRhFh4oRmmJESMNYZFudCEmJQ6HdJaJJyyIbeCJKVwAlTs+fH0WU9kSSluF4qRXHvRLp7BKKJC2HUatEuckupNGU7BYHgiKDoPVKpEUkaZsnkjTJo11OuydCsjHCIy3Whfjk0RUaAxxytchbHCfk0Va/qNImkjyLrRMWEUlaJaTSJJEmKTRty7wSaf9I0h75tBxKlQJOuydqNEWRpkRRo0kyTRJpnzxaIYdCSdJkT5lSSKidHhk03YM3OrTI+0WYpqT0k0RLsmhfVOmT6kq/7CiatCRzVik8c+3Sfkki7T+HLqTZLm9kakk2LSTZnv1UVyoXeb/yqiJF02WeHBG6ukjSOMtI0qeL0ny6pQHVHV/Vd0Ntvi7OVzTpc9FqIK+fYOofPPLCMAzDMHUAq41qDndeGIZhGKYOYLVRzeHOC8MwDMPUATQtWZsFu/LA7bsEzpoXhmEYhmEuDXjkhWEYhmHqAFYb1RzuvDAMwzBMHcBrXmoOTxsxDMMwDFOvCJjOy/ClU6A1RuHz+Qfw70vdMLXrPbj7WHO0DNag79aVuHzMGPR7ZjGmp2Tg/Ts/x7PP3wm9Qo5RP0zEp2+twJtZ8YjXqnDzV/dDpTNglqsVPv1kPsZF5sBako92w67G4hwzZt7aHp1CtRj46gjM/X4XcnvdhUdmb8fQxBDsX70K3V++HenNhiK2fT9Mmrsbe9esxx1tIlFekInynz/Ev19tQNPu7fB3gUV4uLRX5olrzJ/zFfb8tB0Nb74am4sswnNl6aEiRGkU6BwZhIy9x9F4UBMUZ6TCaTXB2bIvlFoDQuIaY/6ubMQ0SsCevXkoOHoE/ZpGwpyXIerpcvd5fE52rEXOpj0wxDREzo5sZB0rQ6PkUKSXO9CtUTjax4QIr5rEYBW0xenCh8WVngpL2l4k6JQoPpCB8Bg9QpNDUHqsDMaUGBgbJwh/F1WDJih1ujz+LsY44fFCFNk8HiOZZTYcL7NCp5Ajs9iCY0Xlwt8lp9iKMLUSujAtLGV24e0SFKGD3VwGbUSw8HdxWsjnxSg8XsjfhXxe3A678EaBNlj4g7jVBri1IeKcLqUWFqfnAqQt3b/Z7hJbhUrt83Ahv5cy+wlvF0oKpRrldpfwdJF787RVqlWwk88Lebco5HC53JDTM6J7Jk8X8mtxVXj8Xbzl5PEiJ88Xl1t4vFTyaHF5PFrUSoXPr0Wt8OwnPxb/ulJ92gpvFuHdcsIbhYaWyeOFkBYHejxaPHmVQu77BefzhHGR50vVXisn6vp5xchPlPv/GJTy0rFSmb+/i8/zxf+40/w/n43Hy6mOPZecbgT/TK7zbGcBAvd396WB8FiqZQpUeNqIYRiGYeoAnjaqOQEz8sIwDMMwzKUBj7wwDMMwTB3gH96jpscHKtx5YRiGYZg6gKeNag5PGzEMwzAMU6/gkReGYRiGqQNqqxhSBO7AS+CMvEyZsho7vxiNpx67HD827SfKfpr2Ke7Y8TO6vvY3llwbimObF2HmwMdQ6nTjYdc6PDTjVsyJHiYkoVPfnYNxU2/AqmY346o7r8UL05aiJD0V60c/hhZXXoNZd3dFM4Maqv97CcOfHADXrc/hoNmO8b/sxM6la9D7lWthKcpGcZ+xePbPVFwztDl2rdyCsqyDcP0xDfqoRGz9cBnWHCvFhCHNYXK6RXtlv3+JTpfFYfe3G7EpswyyXjchz+YSUuJZ648KWXbjwY1QfGQXGlw9EA5zCRRqHbbn2WCITUF040bYujsHbVtGI+/wMZhyjqBjnEHUI2mwfdffQkKeu2knsrccR0RiNLIOF+Ow2YGeTSNR5HChY1wIUkLV4pnpzTmoyEhFmEoB6/5dKNqfgYgoPYoO5CGsUSiMDaORV2KFsUkC1A0aiWepjG8kZN8kkS5XGnz/dNlldiGPziqz4ViRBSFKOY4WlCOr2IpwtQLmUhu0YVroInWwlpNUWgddZDAc5SVCHh0UHSYk0qqwMDjtFiGRVoRFCZm0kBRrg8W5KrTBcKuDRN7idItE9y5tKZEkmmTSlCeJtEwuR7nDJeTRcpVHKl1m9cqmbZR3QKHRCcm0QqkU8ming2TTcpEor1TJ4bS7hERaqVIIyTRJpKmc8iSTFhJqd4VP8kz3oPHKoOk+JEk0JSonmbRPTi0kyp45cylPcmr/4WSSSKvknjKqo/Tup+cv1fGXTQvps8tVKWKtJIWWUPgf5y2nvE/yfJKAV5JJV7VfyKZP8QFMx/qPjJ9OIl1Jbi07/Qfe6dqoVPc8SqRrQn3/3grgGY9KfxvyWiRZAD9EHnlhGIZhmDqAF+zWnIAZeWEYhmEYBpgxYwZSUlKg1WrRrVs3bNq0qdq6n3/+OS6//HKEhYWJNHDgwP/Uv/POO8UokH8aMmTIeb0H7rwwDMMwTB1AX8A0eFLjhLPnxx9/xMSJE/Hiiy9i69ataN++PQYPHozc3Nwq669atQq33norVq5cifXr1yMxMRGDBg3C8ePHK9WjzkpWVpYv/fDDDzifcOeFYRiGYeoAWj9W23S2TJ06FePGjcOYMWPQqlUrfPLJJwgKCsLMmTOrrP/dd9/hgQceQIcOHdCiRQt88cUXcLvdWL58eaV6Go0GsbGxvkSjNOcT7rwwDMMwTD2mtLS0UrLZbFXWs9vt2LJli5j6kZDL5eI1jaqcCeXl5XA4HAgPD//PCE10dDSaN2+O+++/HwUFBTifcOeFYRiGYeqA2iiN5H4GdzSVYzQafWny5MlVni8/Px8ulwsxMTGVyul1dnb2GV3zU089hfj4+EodIJoymjVrlhiNeeutt7B69WoMHTpUnOt8wWojhmEYhqkDyGrA34agJscTGRkZCAkJgf8UzvngzTffxOzZs8UoCy32lbjlllt8+bZt26Jdu3Zo3LixqDdgwIDzci0BM/Iy7vrm2NCqO9beNhkHzQ5M3PQZmlxxDS6flY3UxT/jty63YNC9dyG1zIaHXhqCmde9gb97PIgnXv8F974wGOUFmTgw5AncP2UVvr4mGbl7/kbjviPx4/pj+PrBnohf/h6ue6AH5r04H8YJ72LCH3vQPVyH9fNWC28V13VPIrLZZXhu0X78tXALJvVvhMJDO6ALi8WW6QvQqFsPrDlQiEK7C1cna5ESpEK39jHY+fUatLlrANanFSHT6sRf2Q4YlHLh77L/32w0HZCC5Gv6w1qSB3nXq4UPCfm7/LErG9GNm6FFi0hkpx3DkNYxwlPGVlaIyPJM8Uw0weHI37BVeMxkbT6C3LRCxCeFCo8X8nfpFGcU/iyNQjUIseaL88qO7YEt7V/EapUo3HsUhfuyEZocgqLDxTA2jERYs0RkW13QJjeGKqmZ8KtxGuNhd1eIc+ZbXMJTRC2X4XipFTqFDOlF5cLfhbxrsootwt8lKFwHi8nj7RIUGQR7uRm6CAN0UWFwWEzC40UVGiq8UOSh0cIfRfiJ6MN8viLk70K4NAaUO9wib3ac8HmxkteKUg2ZQvJ2UUCuVMFkP+HnQv4uCqVaeLzQayq32J0+fxeL1Sk8XDzeLuThooBCKYPb6fZ4uJC/jTdPHi+Sv0tFRYUo06kVPj8Xyd9FrVT4vF1oTlt4vvh5u0ieLuTHIvnDiOMUco9Pi9sFlYK2nnum5y09EylP9VQKj2+M5PlC7flLL6W2pLx/uWhLfiIv/Fqq8Hep1jPF5yvjV+a33//cZ8LpFKNVfdCdC4+Mc6FUPds2zqU4ti7s5QPYmuS8ERISUilV13mJjIyEQqFATk5OpXJ6TetUTsWUKVNE52XJkiWic3IqGjVqJM6VlpaG80XAdF4YhmEY5mLCoxqqzbQRzgq1Wo3OnTtXWmwrLb7t0aNHtce9/fbbePXVV7Fo0SJ06dLltOc5duyYWPMSFxeH8wVPGzEMwzBMHeBxt76wgRknTpyI0aNHi05I165dMX36dJjNZqE+IkaNGoWEhATfuhlaw/LCCy/g+++/F94w0toYg8Egkslkwssvv4zrr79ejN4cPHgQTz75JJo0aSIk2JfkyMvHH38shp+koS7q+S1cuNC332q14sEHH0RERIR4SPRwTh7uYhiGYZhAXrB7Ntx8881iCog6JCR/3r59uxhRkRbxpqenC58W/+9pUindcMMNYiRFStQGQdNQ//77L6655ho0a9YMY8eOFaM7f/3113lbe1PnIy8NGjQQc2hNmzYVawC++eYbjBgxAtu2bUPr1q3x6KOP4s8//8ScOXPECurx48fjuuuuw99//12Xl80wDMMw9Zbx48eLVBW0yNafI0eOnLItnU6HxYsX40JTp52X4cOHV3r9+uuvi17ehg0bRMfmyy+/FENV/fv3F/u/+uortGzZUuzv3r17HV01wzAMw1w8aqNA5KJZ80J6cBphobk3mj4iIx0ywvHXkpO7X1JSkjDTqa7zQuY8/gY9ZNjDMAzDMBcbNZ36qUu12MVCnffbdu7cKdaz0NzYfffdh99++01YFtOiIFoZHRoaelZmOrTIyN+sh8x7iIynPsaaHBPue/hdPLPiLQxYLMPWV/tiy5zv0Gv0nViTX44/Bijw6BN9cfjWV7DfZMPdL/+B4vRUFN45Gd1uuQm3vrkSmVsWI/We0UjuORyfPdQL4WoFmm/+CvMf+R5xz7+PDYUWPPbnPiz+ZQ2GPjUQpcf2I7xRe7y49CD6XtUFi+dtRf7+zQhd/x20xig07N4bq/7NxT1Xt0S21YlEnQrOPz9CzzZRaDu2H9btyoPqyjuRYfFIpGeuP4L2Rg1a9E1G/v6taHRdPyh7jBQS3n02vZBJRzdugVU7stC8VRRGtI9H6fH96JloFHJqwrlzjZBJG2JSkLnxEMITE5G9Jx9pJgd6NY1Evt0lJM5Nwj06/lBHkZBIh6kUsO3fhsJdhxEXoUPRviwUHSpGWNMo5OVbENYsCdqUxkJmTTJpl1cibQuKEO2QJDfXbBcyabqX9GILQpQKIZM+VlgunmVpsRXlJJWO1KG8zAZ9jB766GA4zCVCJq2LMMJlt0AdEQ5FWDScNgsUxgiPnNhp98mjCbfGk7eQPNrhkUeTTJry9LxKrE4hkyYptEcGrfJJpBUaHUrKHUIiLZWRRFqu8simbXaXkDw7HS4hkyaJNOVJJu3Ju6FQyIU0miTScoVHPi1JpKncX+ZMMmlJIk1l4l6kvL88mvJeibS0nyTShEf+7JFIS4sAhVzZK0mQJNIEyaTp/ZCOkyTSKj/5gv+vOv9FhVQuteWTPPsJeKlMkjn7f7ZKdU7+vD354/dMJNL+MudKcusz+CyvSiJdraS7mjZOp/I4Exl2TWTW5+qrqq6+9AL4u5a51DovZCVMC4Y2btwoLIVpFfSePXtq3N6kSZNQUlLiS2TewzAMwzAXG9SZq20KVOp82ohGV0hSRdAK5c2bN+O9994TK6JphXNxcXGl0ZfTmenQCM75XOHMMAzDMOcCGo082dzxbI8PVOp85OVkyDCH1qxQR0alUlUy09m3b5+QcZ3KTIdhGIZhmEubOh15oSkeCt5Ei3DLysqEsohkWiS7ovUqpBcnQx2KXkk+MA899JDouLDSiGEYhqnv1HbqRxa4Ay9123nJzc0Vbn5kiEOdFTKso47LlVdeKfZPmzZNhOsmczoajSG3vo8++qguL5lhGIZhzmF4gNodH6jUaeeFfFxOBUWtnDFjhkgMwzAMwzAX5ZqX88WdD03FK0teRXSrXhi6ORrrZn2DFa17o8cdo7BsZAgefbQ3vus2Gjn3T8VNz/6Ohyf0FpLmy266Gde9sQKL7uuK9PXzkdTjavzfb/vw6aO90WXPbNx+Zwf8ee8XWJlXjscWH0V7oxa//bAKxUd2IeThdxCa0gZXDO+BX3/dgslXtRTRqEmmvHXyt2jY/Qrce00rES16dNtIxGuVuLxtFLbPWIz29/SH7uq7caTcgY2lOhF9mSTSWzYfR9srktD0xn4i0rXq8huw3xkqJNK/7cpGVJNWaNoqCsf3Z2JkhwT0SgoVEuk4R96JSNJ/bxAyaZJI5+zKQ2xKqJBJ59ic6JEcJmTSRISzSJxXfnwP7Pu3IUGnRMG/B1GQehxhjUJRcKAIublmhLdIETJvXeOmUKe0EMe7whJh00eJdvLLnb5I0uklViGTJon0oTyziCR9NN8sJNKGMK2QSZNEmqTSNrNJyKRFJGmrJ5K0JjoSLrtVyKQpCcmuMVJIiwm3zuh7z8udnkjWFFHa7I0kTTJpSSJdQpJopVpIpMtI+kx5lVpIpP0jSSvUOpisHtm0Uq0RMmm7jfKK/0SSFnmvRJr2k0xaKpeiSktSaIooTRJpiohNMmmpnKTPUiRpKdK0JIsmmbQvX0UkaUnSTJJnSXJcKV9NJGlJIi1JaP1l1dWVnxxJWpJIny6StKc9b1k1kaQrS6xrJpE+k0jSNfnlypGkz55Ant44Faw2qsdqI4ZhGIYJRFhtVHO488IwDMMwdUFtR09kCFgCZtqIYRiGYZhLAx55YRiGYZg6gNVGNYc7LwzDMAxTB1Dfg2eNagZPGzEMwzAMc+mPvJjNZrz55pvCup+M5sjS359Dhw6dq+tjGIZhmEsSkq3XRrouD2CtdI1GXu6++25hMHf55Zdj/PjxeOSRRyqli5GYVj0wZHsiUt+7Cn999ZXwd1mYUYqV1xrwf51vRc5D72FrsRXXTvoVeXs3wPL4DHS/7XYsfbArDq+diwN3XS88Xr58rI/wKemx5wfMvWsGkt7+AstyzegUqsXP36/EyKcGoPDQDoQ3ao+nFx5AvxG98c41rZCzaw1itvwofFYa9eyPZZsy8cC1rTGmXRQSdSq4572Pvu2i0eG+gfhrWzaCrrkHG8r0wmfl47WHRPvt+qcgJ/UfNLvtSqj63iJ8Sfa6wvDLrizh77Jw8zG0ahuD6zs1ED4zfZJDEe/0+Lu4d64S5yZ/l+Pr0hCRnCL8XfaV2dGvZbTweCF/lpaRQaK+5O8SqVbCtmcT8rfvR1yETni8kL9LRPNo4fEi+bvk213C44X8XezuClj1Ucj1+rtkmezC44We25HCcuHxEq72+LtEaRTC44X8XfTReuHxQv4uhjgjHOYS4fFC/i5Oi0l4vJC3i9NmEVvJ38Xf28WtCfblyd+FvF3I44X8Xeh5kceL5O9CHi7k8ULlpVYHFBodlGqd8HORe/eTvwt5v1AZ+bvIFXLh8SL5u5CHC3m8UJ7KyONF8neRKzzeL5RXKD3eL5K/S4XXw4X8XSTfFsnfRfi9+Hu7UN7ll/f6u0gfXuTvUuF2C48XydNFLv+vL4vk7yIdR/4uBB0j1aFmJa8VyTPm5HKp+GSPF593SyWPlv9+uFK96vxdzpTTebyczt/l5DYq1T2Dc57pOc62jfNJXX3RBfD365lNG9XG5wWBS41GXhYuXIg///wTvXr1OvdXxDAMwzAMc647L2FhYSJYIsMwDMMwNUNey4WncgQuNbr3V199FS+88ALKy8vP/RUxDMMwTABAU421TYFKjUZe3n33XRw8eBAxMTFISUmBSqWqtH/r1q3n6voYhmEYhmFq33kZOXJkTQ5jGIZhGMYLm9Rd4M7Liy++WItTMgzDMAxT28jQsgDuvNRqvc+WLVvw7bffirRt2zZczGx+uTf+/uZrLGzYBf3GjcWKocBTLwzCF53vwK5SG659/Hs8/vwg5O/fjN6jR+GqF5Zgybh22H3zSDTqMwIz56Tih6f74bJ/Pseo+7ri57s+xsq8ctw77xAuC9Pi2heGCol00MNTENGkE4Zcfzl+/mk9po5ojah130BrjMLmF2eiyeUD8MgNbZBpdeLOVkY4fn4HfTvF4p+p89Dp4SHQjnwAR8od+KtEh+mr0nBZmA6bNmSgw5UN0ey2QSgvyISy321IdRgRHN8YP27PxIKNGWjXPhbHUjNwY+cG6JcSCmtJHhLsWXBtWyrOnbtqLYLjGiOyYUNk7sxFQqMwDGwdIyTSvVPChUyaiHQUCEkzSaStuzYgQadE3vYDyNt1DBFNwpC/r0BIpCNaN8Jxi9MjkW7UWhzvDE8WEmkir9yJ46UeifSRIotokyTSh/LMQiYdrvZIpA1hWiGTJom0PkYvZNKSRNpuLoE+LkJIpF12q5BHKyJihaxWFhrtk0j7S6XNzgqxlSTStCWZNEmkZQqFkElLEukykj4r1UIiXVLuEPJpSSKtUOuETJok0kq1RsikSSJNMmiSRDsdLiGRpi3JpEkiTVJo2i9JpKmcZNJULuTPJOt22IVEmrYkk5bKSfosSaQ1/pJoyrsqy6b95cUkk5YkzULy7Cd/luqqFJ52fce5XEImTXWkMkkK7S+nlfL/KT9JIu3ZX/UHaSXZtCSl9tvv325libV/G/7XVHXbVR13oX6tnum6g7M9r6yey6QD+Yv1bBfs1iYFKjUaeSFjultuuQWrVq1CaGioKCsuLka/fv0we/ZsREV5vsAYhmEYhmHONTXquD300EMoKyvD7t27UVhYKNKuXbtQWlqKhx9++JxfJMMwDMNcarDa6AKPvCxatAjLli1Dy5YtfWWtWrXCjBkzMGjQoFpcDsMwDMMEBrxg9wKPvFAso5Pl0QSVnRzniGEYhmEYps47L/379xcxjDIzM31lx48fx6OPPooBAwacy+tjGIZhmEs7vlENUyBTo87Lhx9+KNa3kEFd48aNRWrYsKEo++CDD879VTIMwzDMJTptVJsUqNRozUtiYqJw0aV1L3v37hVltP5l4MCB5/r6GIZhGIZhKlFjmTitcr7yyiuF8ojSxd5xmdN+MG57Yjw2FFowr20Gpna9B3+NfAEZFgcmTbseJcf2I/W21zDiwbuw6OYGOL55AbYMH4mZfx7A78/0Q7xWhaaL3sG3932D0Fe+wN8FFvSLCsLcWfNxwzs3wHXXa4hu1Qv3/7IL19/SB1OHt0De3g0IWzYDG56fheb9BmDJtmxMurk97misRmO9GuXfvonNUxag4yPDserfXKiueRhLsmUwquSYsuwAtm7IQMermyB3z0Y0HT0Sin7/E/4jO0w6fL05A7HNW2Hxpgxk7D6Emzo3QPHRXbgiORSxlgzhb+LYsgTZK/8W/i4Zfx1AVMNkpDQJx74yOwa3jUWfhh5/l1ZRQeIZkRcLjmxHpFqBpCAl8rYdQEJ0EPJ3HhP+LhGt4pGdVy78XXRNWqDI4fL6uyTB7q5AuTYcOWYnFDIgo8SGI8UWGFUKpOWbEaZSCH+XQ7kmRGmUCI4Igol8XuIMMJdaYS0tgSHOCHtZofB3MTSIgtNqhiY6CoqIODhtFuHxIguJ9Pil6IxwB4WJ63ZpQ3zvs9nh8XahZLK5hW9LocUh/F3Ix4X8XkRerUNRuV14vFB5Mfm8qHUiFXvLyePFQj4vKoXweCF/F8qTt4vT7vF3kco8/i9UJvfVUSg93i90vTq1Qni7ePxaFL4ynUohyoLUCpEoT+XC58V1wttFHEdmLF7fFfJ3qaC1Z3K5z9NFqTiRl3w9yN+F3g9xnMtznM8TxusVQ81K9SXPGKlc8oqpzsPF591Cee9Atr8PTCVfFr9jpXP7m3RV5+1SFSfvruqDrKo2qvulWt3ZTvfL9kzUHnX567gu/F2YM4fVRhdg5OX999/HPffcA61WK/KnguXSDMMwDHNqWG10ATov06ZNw+233y46L5SvDuoJcueFYRiGYZg6nzY6fPgwIiIifPnq0qFDh87bxTIMwzDMpUJtlEayWiiOyJONBDc0GNGtWzds2rTplPXnzJmDFi1aiPpt27bFggULKu2vqKjACy+8gLi4OOh0OrGM5MCBA7jo1ry88sorKC8v/0+5xWIR+xiGYRiGOf2apNqms+XHH3/ExIkTRYBlEt60b98egwcPFmF/qmLdunW49dZbMXbsWBHDcOTIkSKRq77E22+/LZaTfPLJJ9i4cSP0er1o02q14qLqvLz88sswmUz/KacODe1jGIZhGObUSAvWa5POlqlTp2LcuHEYM2aMcManDkdQUBBmzpxZZf333nsPQ4YMwRNPPCFUxa+++io6deokLFOkUZfp06fjueeew4gRI9CuXTvMmjVL+MD9/vvvuKg6L3SxVa1y3rFjB8LDw8/FdTEMwzAMcwaUlpZWSjabrcp6drsdW7ZsqaQOlsvl4vX69eurPIbKT1YT06iKVJ+Wi2RnZ1eqYzQaxXRUdW1e8M5LWFiY6JxQx6VZs2YiLyW6WJJO33TTTbgYybS48GHFfLw892m8euWzouzeR2fgmZ8ewR/dH8L4Z+7FbU/+H364HFjV9wZ0vvF2zFqTjvZGLYI/eQLjpt6Azyf+jK3FVlz3xWYMbxCCa764D+a8DORd8xTu+L9tuOfO3lg8ewmmDGsK+Q+vCYnymqd/wJ+78vDaLR2QZ3Phuigzij5/HQMHpmDjlMVYub8AsuEPI9PqxG+HzHh38T5cHqnHrg0HkbvnbzQafTOsJXlAn9uxNq8CoUkt8fmGo1ixMR29uzbAsV37UZyRir4pobCVFSK6+ADsG/6ELiwGmUv/QvqqA4htkoQje/LRokUkhrWLE+e6IiUCrSJ1QkYbZsoQ8uwYjRLmbRvQUK9CQnwwcnccQ1SrSOTvK0RWgQWR7RoLmXS+3QllwzYocbiERNqk8kQWzyl3Ir3ECp1CjoNF5UjLMyFEKceBnDKEqxWI0apQVmiBISoIwfEGWEw2BMcZYCsrhaO8BEGxEUImTRJpdVQ0nHaSR8dBGRkrpMUyY7RPHk1bp9og8ia7JyQFyaPLHR55NKUiqwMKlUceXWQhKbQWJTYHSiwOyKm83AGlVx5tsjnFMSSRNpE8Wq0SMmmP/NkjiZak0CSTdrsqPOV2jySaykkWLVfIoVYr4HI6PfJoknU77D4ZtCSRlqTPGqVcyJlJEi3JosXW5ZVH+0mlJRm0JJEm6P0jibSQNssAlVdOTVtq1ydLdrkqSaGpzFvVk/crl+TR/kPS/vJnKvbIsf1lzpX3nwpq17/O6T6EKsmtZTX7EDtbifTpOF8SVVk9l0cHsHK3RshoIKCWSfJeo+9gKU2ePBlVkZ+fD5fLhZiYmErl9Jo6IFVB5aeqL23Pps0LblJHQ0M06nLXXXeJ6SF6SBJqtVosAOrRo8f5uE6GYRiGubSocHtSbY4HkJGRgZCQE35XGo0Glzpn1XkZPXq02FIogJ49e1YZnJFhGIZhmAtHSEhIpc5LdURGRkKhUCAnJ6dSOb2OjY2t8hgqP1V9aUtlpDbyr9OhQwfU+bQRzaNJdOzYUSiLTp5nkxLDMAzDMKdGVuGudTobaIakc+fOWL58ua/M7XaL19XNmlC5f31i6dKlvvo0mEEdGP861A8g1dH5nIlRns16l6ysLERHRyM0NLTKOV9pIS/NqTEMwzAMc/6njc4GkknTLEqXLl3QtWtXsRzEbDYL9RExatQoJCQk+NbNPPLII7jiiivw7rvv4qqrrsLs2bPxzz//4LPPPhP76Tt/woQJeO2119C0aVPRmXn++ecRHx8vJNV13nlZsWKFT0m0cuXK83ZBDMMwDMOcH26++Wbk5eUJUzlaUEtTO4sWLfItuE1PTxcKJAlaIvL9998LKfQzzzwjOigkgW7Tpo2vzpNPPik6QBRCqLi4GL179xZtkqldnXdeqOdVVZ5hGIZhmBpAaiGvYqjGx9eA8ePHi1QVq1at+k/ZjTfeKFJ10OgLGdReSJPaGvm8UI9q7dq1layGqfd22223oaioCBcjj63/HM+M+gq3Z7ZHy2ANJm76DBpjJJ6t6I9Hn/wEr+q3wFKQiTk97sQvewuw6uEuGBZrwKgfJuKTyctwYMgTyLQ6cFvXeGz66RcM+vkVbGt3O5pccQ3u+HQjNv+6AJM66FCWdRAFb0/Asuf/QM+R/bHgSLGQE1+pPIJeETpkvPsK1k1dgTaTHsCy9BIhn/56ezYSdSq89+de7Fu/G+3v6oq81A1wmEtgvew6aILDsfBQKT5dexjJ7Vvg7w0ZOL57N27r1ADF6amiXljWNiH1LV/zO9IXrkVoSlscXXkIBw4UokPrGOw32TGsbRz6JIeL62kZqUVw0UER9dm1d6OImk0S6Zx/9iImyYjotlHI31uAqHYpSC+0CIm0tnkH5Nu9kaQjG8Hl/b/JMjmhlstwqNCCAwXlXnm0CXuzShGlUSAjz4yoIBX0Md5I0vEGIZG2lhRBHxcOm8kbSTrBE0laSKSjEoTEmLYwRntkxvpwn1TaodTBZHd5okfbXT55dLHVKco8EmmHKCu1OVFm90ihSR5daLILiXQxSaa98mgpkjRJpG02TyRph80lEkWMdlBUaYokLaJHu31RpV0uty+StNvlFjJpkjeTJJrk0XQPlJciSUsSadpqTpJHSxLpkyNJS9Jlkj+TTPpEVGlPJGkpajTtJ5lsdZGkieoiSfuXS/jPDEt5SSItlVUlka4qkrQU6VqKJO27Hr//Uf+p6NMFnJNXc+yZSJhll3gkaaYeThvVJgUoNeq8kNOetDB3586dYg5t2LBhwqyG8gzDMAzDMBeFVFqCOilkK0z88ssvGD58ON544w0RJ4E6MQzDMAzDnBqP0VzNR09ktZlyCsSRF5JbSYEZly1bhkGDBok8LehlqTTDMAzDnAE8bXRhR15oJTFND/Xq1UuE0qYolcT+/fvRoEGDml8NwzAMwwQKdSCVDuiRF4omqVQq8fPPP+Pjjz8WmnBi4cKFIvokwzAMwzDMRTXykpSUhPnz5/+nfNq0aefimhiGYRjm0odHXi5s54UgF10yqklNTRWvW7dujWuuuUbETWAYhmEY5gw6H27uvFywaaO0tDS0bNlS2Aj/+uuvIt1xxx2iA3Pw4EFcjPSbXYzrOsZi3gef4oa0NRiwWIZlM8bio9c+hEofgveveQ1Pv3gP1uSX4/buCdh05TBcvfpTzIkeJnwwbn1zJe66sSV6/fwpVDoDZqu6YNz76/DRgz3w759zYSsrxMFJDyO+82DMnbYay3LNmHFDW+gUMgyKMWDPCy+j7309sWLmZqzMK8fRRgNgcbmF58ync/egb6dYpG3cisJDOxA/9kG47BYERcRjzu5cRLbojhmrDmLLpmO4/vIUZO3ZhrLMg+gapRD1lFoDChf9CkNsCg7N34gjK4+iQdNo7DlagoNmB4a1jkGezYleSaFoElwhPFm0mf/CvmON8Jcp3LgRjQ1qxDUNR86ObMR2iEFU+0Y4XGpDePsWyLQ6kW93QpbYUlwz+bvkOz1BOamt/QVmGJRysd2TSd4uSqRmlSKT/F2MWpQVWhAcb0BIgxCYS8vF1pAQKfxdgpOihU8N+buoouPhtFngslmgJJ8X8kUJjoRL5/F2IY8Xq9wTLbXM7kap3e3xeXF4tnKVGkUWB5QaHRRqrcjTtqDcLrxdyMelwGQX/i4KtQ4l5XYodQaRN1mdUGnUPn8Xz9YJp4PycrElfxcq9+TJ88VTTmUatQIupxM6tUL4u0h+LZK3i06lEFuX1/+FtlRX5/WCUZ/k+SL5vKi8hiHk7yL5pBBSnvaTvwt5u1CZVF94wnjDdNDfr6eNE54vKrlcJE9dmc/fRfJ+EXk/JxRqVjq3z88FMl9eeL5Ieb//O/9rPhN/l6qo7rjTHV+V10pNvV1OdZ7aci5a9X/fLhT+7z3D1IvOy8MPP4zGjRuLMNwkj6ZElsIU04D2MQzDMAxzcQVmRKBPG61evRobNmzwxToiIiIi8OabbwoFEsMwDMMwp4HXvFzYkReNRoOysrL/lJtMJuEBwzAMwzAMc1F1Xq6++moRPXLjxo2oqKgQiUZi7rvvPrFol2EYhmGYMwzMWJsUoNSo8/L++++jSZMmIlQ2hbymRNNFVPbee++d+6tkGIZhmEsNdti9MGte3G433nnnHcydOxd2ux0jR47E6NGjxSp8Uh9R54VhGIZhGOaiGXl5/fXX8cwzz8BgMAhX3QULFgivFwrMeLF3XHYv+gNxCxaj223/Q9vn1mLdrG8gf+I2NLhsEH6aNhZmlxsTLUvx8N2d0HXxPHy34Tge3hmMJ17/Bfc8eyXS189Hk69+xRNbZRh4xwg8M20ZDqz8Dd0Pz4NCpUbTfsPx03e78NTd3bCjxIp4rRKRSz/AVR1jcfmrI7DwjwOIfeRFbC6ygtSqLy/eh+7hOlxxZQqObFqHjo8MR0m6xzNnr7YJQho0Q1zbrvhyWRrad22A/f8cRt7ef3BTm1iYco4ICS42/gZNcDiMDZrh0IJtiG3eBkf/ysDOHDOGdknAkXIHCu0u9Ew0CnlzkqwEioMbEaNRwvrPcuT9/Q8aRQche+MBRLeLQmznBBw7VIyYzi1g7NABOTYnNK26ijYsrgpYQ5NEO3T9R0usQgZuVMmxL8+EMJUCu4+XYm9WKWK1CuTkmlFaYEFIg2CUFnq2xuQw2EryEJwYg+CkGCGRDkqIEzJph8UEZUySuC+R9BFCYktbSoTZrUCZzSVk0aV2F0x2F+RKNQrLHUImrVB6pNJypUqUi7xKLWTSJJFWkjza4hASaZJNF5c7oFRrhNTZRhJrtQIqjRJO+wkZtCSbdtrdPtm02+mGy+WRTbucbiGTFpJnrwxap1aKvLSlRPJnkkf7S6H95dFCNu2VU5OE2SOVdnskz253JZmzv4Sa9vvycpI8y4REWirz1DkhV5Zk0yStJXk0lXvyskr7CcpKku3KcmXPi0plsv9KfqVznkx1Mml/ubKshtLl6iTPNZVIn4k8WmrjTOTW/tdTXyXSBMujz21gxpqnCgQqZ9V5mTVrFj766CMsXrxYdFrmzZuH7777TozIMAzDMAxzFvC00YXpvJCXy7Bhw3yvBw4cKH6ZZGZm1vwKGIZhGCYQ4c7Lhem8OJ1OsTjXH5VKBYfDUfMrYBiGYRiGOV8LdkkSfeeddwqfFwmr1Sok0nq93ldG4QIYhmEYhjkFbFJ3YTovpCw6GYppxDAMwzDM2VFbi38Zd17OjK+++ur8XQnDMAzDMMz5Mqmrjzzz2gT0vPMDrBzkQPqmpeg5ajQ+/H43/nl7CBp+MhFPzLwTU2+aDrz2Nfp/tB3XNg3HN+/NQnF6KorvfgtJPa7G0I834uuPf8X3NzZHzq41MCa1xLIx09Dlumvw0X3dhbR4XAMzOoVqce3IZlj5+A/oNvk+yG99FgfNdnyfLkOsVolBDUKwcuEO9PpfR7R6/B6UF2RCNvxhqPRGhDdqj6mrDyKlU0cM6tMQh7fswvgrGiN/32ZYirKRWLJXyIAp4nTG7wsR1qg9Elo1xb5/stCuXQy2F1uRYXFgeMsYmJxuIWuOMqcLSTNS16Js3Qo0MaiQ+dc2HN94SESQzt6eg7guDRHTtRUOmx3Qt+sEVbPOQiLtjGkOu9sjxztW5hDt6RRy7Mk1wahSIFKtxO7jJUIefSCzFIV5ZoTGGUQkaVOxRyJtKS6EsWG0kEdT9G3aahMT4bCaoYxNEpGkSU4sC4/zSMABuLzyaLs6WESP9o8kTfdfanUhv9whZOr5FB1a7YkkXWj1RIwmKTRJpFVaAwrMdhSabSKCdHG5XUikVVotLFankDurNUqfJJqk0HYbRZhWwm5zCdm0kE9TJGmSUEtRpe0OnzxaiiRN0aFJHu0r90qfKeKziDTtlR3TfpIz09Ynm/ZKniVJNMmjCalMkkdL8mMpkjS9H1IEaTpOkkhL0ZwlKbSEVE4yaalcRIQ+Sf7sOQ5VRpj2RZU+KZK01N7pIknTIv/qJNJVcSaRpM9GplybY2rbxrmUR9dlJGnmHEH/57VNAUqNAjMyDMMwDFNLamvxX8E+LwzDMAzDMPUCHnlhGIZhmLqA1Ub1c+Rl8uTJuOyyyxAcHIzo6GgRK2nfvn2V6pAU+8EHH0RERIQIS3D99dcjJyenzq6ZYRiGYc4FtQsN4A5otVGddl5Wr14tOiYbNmzA0qVLhdndoEGDYDabfXUeffRREYZgzpw5oj65+V533XV1edkMwzAMwwTqtNGiRYsqvf7666/FCMyWLVvQp08flJSU4Msvv8T333+P/v37++TaFMGaOjzdu3evoytnGIZhmFrC00aXxoJd6qwQ4eHhYkudGBqNoRhKEi1atEBSUhLWr19fZRs2mw2lpaWVEsMwDMNcnGqj2sQ2qkCgctF0Xigy9YQJE9CrVy+0adNGlGVnZ0OtViM0NLRS3ZiYGLGvunU0RqPRlxITE0X5rRs+hEpnwFs9HsQ70x/H8sEVGDOwIbb27IspU//CrIZ3wFVRgcHPLMTmH79F/zVzhG/IZTfdjGtfX4Efnu6HjbN/gqUoBwfuu134vtx9z1WYd6wUP4zpjK5H/sTwhmHY9chEXP1YP7Sa/AYWHC/F4dYj8eLSg2gTosGUH//F4B4J6PHMVcjbuwHJE55CVrPB0Ecl4uvt2Yht0xste7XBqlWH8L9BTXFvj2QUHdmFPlEVcFpNUJJnyR/fITi+MWJadsGB+fvQsG0i+ndpgF2lNtzSJVF4zZAvS3ODC2q5DFHkYbJlKVKC1MhfvRrH1vyLxJaROLYhA8d35yOuazMcLLIismt7aNt0R77dCXmjjnDENIerAsh1eWJZUVu7fd4uCmzPKEGMRin8XQ4fL0OUUYviPLPwdwlNNsJUbIalKBfGhrGwluYhOCkawcnxcJSXQp2QDGVMIlw2C5QxScIThXxBXMFRvvfRItf+19vF7kJBuQNyr7cLJXqPiizk7aIV3i55pTaxlfxdyPOlxOvtQvlikx1Wi0P4tditHj8XlVYBh80Jtaay54vwc3G4oFTLfXmdVgmX0ymumbxb6B6CtSrh7yJ5u1BySfuddp8XDPnAUPJ5uyjl0Co9/i/kzyL8WtxuaMXW49Gi8db193ahJPm5UJnkNaKQURsnvGIkVHLPv7l/OXmE+Pxa4MlL3i5VebRU8mvx83aRiiXPETq2Km8XTxv/9Xbx90mp7jicRRuV6lZR5u9NUx0ne8icK4+Yc+nxciGRTskeL+eBChfgrkWq8PyvBiIXTeeF1r7s2rULs2fPrlU7kyZNEiM4UsrIyDhn18gwDMMwTN1zUUilx48fj/nz52PNmjVo0KCBrzw2NhZ2ux3FxcWVRl9IbUT7qoKCRvoHjmQYhmGYixEaaZXctGt6fKBSpyMvFKWaOi6//fYbVqxYgYYNG1ba37lzZ6hUKixfvtxXRlLq9PR09OjRow6umGEYhmHOEbWZMnJ7U4CirOupIlIS/fHHH8LrRVrHQmtVdDqd2I4dOxYTJ04Ui3hDQkLw0EMPiY4LK40YhmEYJjCp05GXjz/+WKxL6du3L+Li4nzpxx9/9NWZNm0arr76amFOR/Jpmi769ddf6/KyGYZhGOaSHnkpLCzE7bffLgYNaNkGDSSYTKZT1qfBhebNm4vBB1IFP/zwwz4VMU4KzOqfarLWVVnX00anQ6vVYsaMGSIxDMMwzKUCRYOnVJvjzxfUccnKyvIZyI4ZMwb33HOPmC2pCjKQpTRlyhS0atUKR48exX333SfKfv7550p1ya9tyJAhvtcnK4rrldrofPPW60uxfebdiNIocO2SNzG16z2InT0PP+3MxbCEEDz9zGd44ocHkbVtGRIuG4ahP2dh4lP/w9Lx3XF47Vw0XfQOgiLi0eWGG/D1T6mY8UhvvNpJJSTQsk+fxspx09B3xv345c80hD0+DXNKohCuVuD+H7bj59+2Ycj/2uPQumXo9MI90N7xrJDtrnbE482VB5FyWXd8Ni8VgwY2weODmiF711rc0S4Wza2HxLU7l88S545o0gmpszciqV1bdOgcj805ZtzSKwU3d0hAicONPkkhor5RJYds5zIk6lRoZlAjc/k6NE8KQcbqvTi+KQsNejbGkcPFSDPZEdqtBzIsDqjb9oY7uQMsrgqUGhJwtMwBUtXuL7TAoJSLe/k3s1TIpGO1SqQeL0FikApRsQYhkQ5rFIqyIgvMhUUwNoyEtSgb1pI8GBsnwGEuhT4lGcr4hnDaLFDFp0Ae5ZFIu4JjfNJcp87j70OU2NyQyRUosblQZHEKqXSe2SOPVqp1yC93eKXSOuSTJFqSR5vsYkvlBSYblDpPmancIWTQdpKS21xCIk15lUbhlUW7RZlS7ZFIk2zaaffIozUaJZx2h0/yTPJokjwbtCpfGcmjJfk0JbonKheSZ5dfnsq9dUkS7ZE2u6FRKoT8mfYr/aTSJIUm/OXR1J44zuWVWMs99alMktKSbFrUdbtATZwsf6b9Uh3a+OTPleTK/5Ulizp+/1d0PkkefUJ67Ve3Cn3tyfLo00lwT9dGpbrVtHEu5dFnK5OurxJpJnBJTU0VJrJffPEFunXrht69e+ODDz4QIyTUGakKsjj55ZdfMHz4cDRu3FgYy77++uvCId/pdFaqS50VmkWREg1SnC0B03lhGIZhmIsKUgvVNgH/MWYls9baQCaw1MHo0qWLr4zMYuVyOTZu3HjG7dCUEU07KZXK/6x3jYyMRNeuXTFz5swzmoW5KKXSDMMwDBNwiA5ILaZ+3J7Oi2TGKvHiiy/ipZdeqnGzJJ6hUD3+UAeEhDPVGcSeTH5+Pl599VUx1eTPK6+8IkZlgoKCsGTJEjzwwANiLQ2tjzkbuPPCMAzDMPWYjIwMMcIhUZ3X2dNPP4233nrrtFNGtYVGf6666iqx9uXkTtTzzz/vy3fs2FEEYn7nnXe488IwDMMw9QFpbVttjieo4+LfeamOxx57DHfeeecp6zRq1EisQ8nNza1UTutWSFFUnUGsRFlZmViMS/Yn5OFGXm2ngtbU0AgNTXWdjcEsd14YhmEYpi6g4IruCxdVOioqSqTTQV5q5GxPwZHJLJYgI1mKQUidjVONuAwePFh0QubOnXtGC3G3b9+OsLCws3bG584LwzAMw9TjkZdzTcuWLcXoybhx4/DJJ58IqTS54d9yyy2Ij48XdY4fP44BAwZg1qxZYuEtdVwGDRqE8vJyfPvtt77FwwR1mBQKhVAeUXgfMpmljg3JsN944w08/vjjZ32N3HlhGIZhGKYS3333neiwUAeFVEZkFPv+++/79lOHhsL1UGeF2Lp1q0+J1KRJk0ptHT58GCkpKWIKiTzbHn30UaEwonpTp04VnaSzJWA6L3cNb4rtnXpjTOqfeChuABrr1eg94WesntATDV6YCuXtX+I949XoPboL3r+xPTpf8yQWfjQMu259CI36PIjPJz6BiX/Mx4PdGuDd52W44uAv2P7hz7j5pWH4/sUF2G+yIb7dzShxTMbE+fuwZl06vuqbjBmLVsGcl4GUea/COXwK0ltdg3lbspHQqR9e/mM3jqbmYNK47njq2c/x0yNPIVlZBoe5BMHb5yJ37V8IS2mD1K/nI7b1eMQ1DMO2Pwow9KVk9GoYjj+sTtzZLBKx8nLoFDIEp29CjEYpfFhylyxFm0gdwpuGI33NYST2SsKOX1Nx3OJE38u74uC0NTA53ZA16yY8YuxxrZBr9mjxDxbZcLjYAqNKgS3HS4S3C/m8zDtahNFBKgQbNSjILhPeLkGROpQVliK8SQTKC3JgLy+BsVsCrDvz4LJboUlsDYdlG1QJjSEPjYLbuRwV4Q3g1hnFuVzBJ1a0F1ldwttF8nfxeLs4UGJzQqnRIddsR4nVCYVaizyzDSXlDqh0BmQVW4W3i6hfZhXeLpQvJs8XjRpWi8Pn42K3OH0+Lg6bExqtCnKlHOZSG0IidFAo5WI/1aWt5ONCW/J2CfZ6u/j7tdBWo5TD5a2rVspFXXGcwy7ujcqkX1lUl7xdyMNF8nYhjxZf3mskInm70Jag+pIpFXm7eMpOeLtQmS8v/GM8dRV+xiSSd4vk7SL5wJy8X6rjy1fh7ULbyr4wp6YqLxn/407nD+PfxoXydqkp9d3bhW1lLhC1dcl1nz+TOlIWVWdIR1BnxF/iTE75p5M802iOvzldbQiYzgvDMAzDXFT4ebXU+PgAhU3qGIZhGIapV/DIC8MwDMPUARdzbKOLHe68MAzDMEw9dtgNRHjaiGEYhmGYegWPvDAMwzBMXXARq40udgJm5KXg5c+xLL0Enabuxeh+yZi46TMUHt6BA/dNR4+p2/Hxm2Pw+oufYvEN0dBPfwiRzS7D7KFP4Yu5+/H7M/2QaXXg6bhM5D99J0bf3Rl/3PkBvltyCPY7X0NqmQ2JOhXGfbEJI1tH4bcfVuHQ34vQeeoLMOUcgSY4HD/nGRDTtg+emLsbn/+6G7dd0xK7V21Azs41GN0uGuUFmWiUvwWW3z5CcFxjpH3+HXZ/uwkNO7fBlrUZ6Hd5CsZd0UhIsu/olIB+ySFQyICE0jS4N81DSpAahYv/QFujBi2ahePo8r1I7pOExCtaYO/hEiT074o9pTZkWBxQtuuDQrsLFlcFCrQeqXJakQ07c80wquT4J7ME/xwtQrxWiX8OFyIpSIVEowZZx0sRmRiCiCZhKMkvR3jTMIQ1j0N5wXGENkuEpSgb9rIi6Bs1gt1cCofFBFVSM7jsFiAmBa6QWCGxpa1N45FKFzsgpNFC2mz1yKMVGh2yymxCHp1lsiHXbBPy6ByTDTmlHil0drEVuVRHa0Ch2SbKVHojCkgerdVCo1PBZnFCrVGKrc3igEanhM3qgN3mglqngsPqkURTudNO8mklNCSRtjtg0CrFdXvk0Uq4bJbKsmmnHUFqhZBH01aSRdOWkpBSe7dUV6uQpNJuIWmWJNNKb7mGZNMKuUce7d36y6MpSRJqSd4syZUVXl2ryPuVVyV/PrH/hByW9kttVJI+y/4rj6Yk1RFya985TkiP/SXI/m1UJauujTy6pmres5FIn05uXandml3OSedjeXQgQZ8HtU2BSsB0XhiGYRiGuTTgaSOGYRiGqQt42qjGcOeFYRiGYeqCilp2Xiq488IwDMMwzAWktutWKnjNC8MwDMMwTP2AR14YhmEYpi5gk7oaEzAjL3c89D5e+/NZHFg5D8bv52HAYhlenTwetz7yCXbO/wn9V70LbVgMFne7ER9O/QvfvTICGwotaG/UIviTJ3DXjS2xaPDDmPn5FsS8/Q1W5pWLqMu3zdyMEclG3HTPZdi1ZAF6f/oUCg/tgFylxjJFKyG5bnZFf0z+YQeuHdEBGxZuxNENy/BorySUHtvvkdLOnY6giHgc/ugjbPt4ORpe1gVbFqRh3aEi3DmgCXaV2nBvj2QMbx4h7qWJ8zjk//wh5NGlC2fj2NzFaNc4FGnztqJRnyQ0vLIl9u7JQ9LgyxDZrx+OlNuhuWwQcmxOEUm6xNgQrgoIqfWu3HIYlHJszCjGhiOFiNeqsC4tH5sPFiBFr8aR9BLENQgW0amLckyIbB6O8OYxMOelI7xlMsJbJAt5tKFpE9jLvfLolJYembHTDldoglceHQebPkpcf4lLiUKLJ4I0bUkeTYlk0SSPVqp1yKa8Vi9k0lklJ+TRlKcI0lklFuR6ZdO5pTaog/RC6mw1O0SkaJJAkyxaTfJoi0PIpamM5NEUTZqiStttVKYQ8miHzY4grdIjkbZZhDyapM90HwZvJGlKwRqlkEdTorp0b7SV8kIq7Y027S+P1igV3iFiKaq0R/qsVUrRoT0Rpgkqq0oe7R8p2hdVWk4S6xORqaUI0pL0maDsyTJn2n9yhOmTpcFSG/7HiTo1lEeffPyJOrIzliifLoJ0tcdVcZ3nSh5dW4Uxy6MDGGnBbm1SgBIwnReGYRiGYS4NeNqIYRiGYeoADsxYc7jzwjAMwzB1tualFutW3LzmhWEYhmEYpl7AIy8MwzAMUxeww26N4c4LwzAMw9QBkgqwNscHKjxtxDAMwzBMvSJgOi9hjdpjRFoLPPXaBPQZ9xHWzfoGY/d/JbxGut7yP7zz+G+Y9dYozDtWipQgFVr88jLuvqYZRn33CD6ZvAxNvvpV7CNvlJFf/IOr44Jxx9hO+Oe3P9D/6yeR8NqnYuX3+ugrENGkE5pdMQhPz9qCq6/vgZdu6YC0tUvxwsDGwgOGvEPUiz8S3i5RLbpjy/T5aNitFzb9uAt/78nH2KHNsbXYikyrEze3joLdXYE28jxots0T3i7mRd8h4+ff0blRKPb/uhFpCw+g8ZBW2LcjF42u7oaYwVdiv8kOXc+rgdZ9UeJwoySimc/bZUeOWXi7RGmU+OtQAeK1Sqzen4f1B/LR2KDG/kNFyDtWiqjWkSjILkNUq0hEtYmDOe8YItqkILJdE9hK8hHSoim0TVrBbi6BOqUFnBaTx98lItn3i8ARHCu2pdCiQPJ2sbqQX+6EQvJz0eig0hmQVebxdqGU5fVwOV5oQVaxFeogo/B4IW8Xld4ovF0KS23Q6DQoN9uFt4tap/J5u2h0StgtTmh0KrElbxdRZvPktTqV8HYxBKkqebuEBql93i6SV83J3i7C88VhF54u0lbydtGrFF4fl/96u9B+ykveLrSffFzcXs8XcdwZeLv4l5/s7SJ5s/h7uyj8/sv9vV2kcjpcsvuQtpK3i3Ru/w+K6rxdJH+U6rxDzqSN8+HtcipO18b59HapK38XOi37u1xc4QFqkwIVnjZiGIZhmDqgwl2BCldtYhtVIFDhzgvDMAzD1AHUcalV58UVuCMvATNtxDAMwzDMpQGPvDAMwzBMHVDbdSsVvOaFYRiGYZgLCU8b1RyeNmIYhmEYpl4RMJ2XLW8OxMrPv8TDB74Ur3uOGo1X7/sec96/B6vuiBUS5PY/v4T7b2yJcb88jfeeX4Cm3/+BObHDRf2hH2/E8AYhGHN/N2z46VcMmvM8GrzzjZC2/pUwCA/MS0PLgcPwyJebcP0tffDmqM7Yt2Ih3hzaDEO1x4WMWLfoQyGPjmnbB/+8/Ssa9+iNy69sjZXbc3DfNa2wodCCDIsDt7WNEfJoo0qOoG1z0VivhmneV0j/4Wd0aRqGvbP/xv4/UtH0mrbYtSUL27NNiL16KFLLbAjqMxJoN1DIo4sjW+CQTSvk0VuzT8ijVxzIF/JokoSvSc1Fs2CNkEfnpJcIeXR+ZimKs3KEPLo06wii2jcS8mhLUQ5CWzeHtlkbjzy6STsoklp6JMXRjU/Io0PifM89r9wp5NG0zTU7hDz6WKkN6SUWIY/OKLH65NHpReVCHq3WG3Gs0CLk0ceKypFFdfVGZBVbfPJoc5kdFpNdyKOt5R55tFav8smjtUFq2CyOKuXRTrvNJ48ODVL55NHGIDUMGuUp5dEi77CL11Ldk+XRWoW8Wnm0JIt2e8skeTTlSQp9JvJoojp5NBVXJ4/2HFe1PJpku57jTpyjKmmzPyfLo6uS356NxPpilkef6prOFJZHM9WNvNQmBSo8bcQwDMMwdQD9cHFzVOkaETAjLwzDMAzDXBrwyAvDMAzD1AEVFbVUG1XwtBHDMAzDMBcQVhvVHJ42YhiGYRimXsEjLwzDMAxTB/DIS83hkReGYRiGqavAjLWKKl1x3q6tsLAQt99+O0JCQhAaGoqxY8fCZDKd8pi+ffsKuwL/dN9991Wqk56ejquuugpBQUGIjo7GE088AafTedbXFzAjL7+1GYBrp36LF+66CWuz/kXTgq34ebIWSdMfxC8/7cK4ldPxxGX34/G8XZixKxshSjkGTFmLozv2YOtzgzBl9k8Ysvh9FCd2hWL4y/jV0Auzv9uBjiNG4qGP1iMvbQ8WzrgXfW58DhuenwLV4Y0e34yf3sC+1TsQ33k01r30Flre/ibaNonA0tk5eOyTNugUH4LpVieebBeNxyoqEK5WQPP392hm0CApSIlD38xGt9aR2D3rLxQdKkanB3rjjykrkWdzYuCIEUh9ayUs1HtvPxgljieRF9ZUeKqo5TKsP1aGjBILYjRKLNmbK3xdqP1P9+Tg6QgddJFByD5ajNgO0cg9VgJ7WSFiOiWhdO8h4UsTPawFrNtyYGzTCvKwaNjKlkLT4iq4g0Lhsi+CO6oR3DqjeL5WQ4zYkqdLttkBuVIt8lkmu/BuIT+XEptTeLuQx0uZzSm8XY4WlgsPF4VSjWNFFmgM4ZCrKF8OdXA4soqtcDpc0Oq1wtvFk1fDWi7lVbCaHTCEaqFQyFFWaEFYjF7k/b1dyG+FPF3I28WT93i7GLQqqBVyn7eLWimHU5R7fGAIyduF3k+dSiHqEkEqhc/bRfJgId8W8nGRvF3Iz4XQ0DncLiFtlLxdqJ7kr0LHSx4gp/N2EXlvXcm3RfJ2kTxdpP3+dcT7c9LxkreLRHXeLv6eKP7eLlUddyb+MP74+81Uxen8WC5mbxfPOS+swYr/6djb5eLG7XKLVJvjzxfUccnKysLSpUvhcDgwZswY3HPPPfj+++9Pedy4cePwyiuv+F5TJ0XC5XKJjktsbCzWrVsn2h81ahRUKhXeeOONs7q+gOm8MAzDMAxzelJTU7Fo0SJs3rwZXbp0EWUffPABhg0bhilTpiA+Pr7aY6mzQp2TqliyZAn27NmDZcuWISYmBh06dMCrr76Kp556Ci+99BLUajXOFJ42YhiGYZh67LBbWlpaKdlstlpd1/r168VUkdRxIQYOHAi5XI6NGzee8tjvvvsOkZGRaNOmDSZNmoTy8vJK7bZt21Z0XCQGDx4srnn37t1ndY088sIwDMMwdcC5WrCbmJhYqfzFF18UIxk1JTs7W6xH8UepVCI8PFzsq47bbrsNycnJYmTm33//FSMq+/btw6+//upr17/jQkivT9VuVXDnhWEYhmHqMRkZGWJhrYRGo6my3tNPP4233nrrtFNGNYXWxEjQCEtcXBwGDBiAgwcPonHjxjiXcOeFYRiGYeqxw25ISEilzkt1PPbYY7jzzjtPWadRo0ZizUpubm6lclIEkQKpuvUsVdGtWzexTUtLE50XOnbTpk2V6uTk5Ijt2bRLcOeFYRiGYQLA5yUqKkqk09GjRw8UFxdjy5Yt6Ny5syhbsWIF3G63r0NyJmzfvl1saQRGavf1118XHSNpWorUTNTxatWq1VndS8B0Xo6UO/FV8Bps6hSHwpuuxtQt2ZhwaAkeiukLnUKG+bsjMcCoRf8XliIn9R9kzrwLL7z5g5D8Fs94G0HrPsCbWfFYMHcDrrhtJJ6ctgIl6anY/tMktBg8UUhO26XNg1KjQ8HbE3B8/QE06vUoVj83CWkmO8b82haLvirAqze2Q8vIILxqc+H5ZDnkGRsQT1LcudPRKVSLhDAdUj/6ET16JiCieTT++eFfDHzhKnwz6Q8UOVwYesNtSH1xEezuCthaXwmT0w2FDNhW4BL3sepIMY4UliNRp8LcnVk4VliOcUYNpuzOwZXxBgRFBCH7SDHiOsdCHx2M4mPpiOvWBGX/pHnk0de1hnV9Dpw2C7SthsNu/h3qFpcJSbTb+SecUY3h1nrk0WXaSJTb3R55tMkpnhWljBIbFBqdkD8fIim0Vo8jxRaUWh1Q6UNwKM+MMqsTmuBwHC0o98mjj+aboQkOhVwhR1GJVUiiLSY7nHaPJNpissHpcEMbpIK51AaXy43gMB1K88uhjQ+GQimHzeJAkE4FnVoBh7UcEQa1uC96f4xBajit3rxOJSTRoTqVTx5tDPLkSRZNeUkSrVcrRZ5+IUnyaEkWTdsgldwneab99H6QLJrk0VLUVyF59uZJQi0dL0mWNQqFkDT7t1uVPPpEuacuHV6VPNpfEixlqb50ndXJnKuTR1clvz15tb907Okk1rWRR1d1juo4G1m07BKSRzNMbWnZsiWGDBkiZM+ffPKJkEqPHz8et9xyi09pdPz4cTElNGvWLHTt2lVMDZGMmhRJERERYs3Lo48+ij59+qBdu3bimEGDBolOyv/+9z+8/fbbYp3Lc889hwcffLDaqS4EeueFYRiGYS4mLmaH3e+++050WKiDQiqj66+/Hu+//75vP3VoaDGupCYimTNJoKdPnw6z2SwWEdMx1DmRUCgUmD9/Pu6//34xCqPX6zF69OhKvjBnCndeGIZhGKYOoGkYSrU5/nxByqJTGdKlpKSgouKEwy91VlavXn3adkmNtGDBglpfH/u8MAzDMAxTr+CRF4ZhGIapAy7maaOLHe68MAzDMEyddV5ctTo+UOHOC8MwDMPUAVJ06NocH6gETOfliTUf4Nme4zEhZycmR7ZBY70aPWbsw5TOcWh5Uxckv/cFvtn4Fe658TNojVH4pen/ENdxDZJaxuHa11dg4mM3Yuq7c2DOy0DJqncQ/PHnUKh1CJ3zOgwxKQhNaoE1D76BdqPfwdw3H0Cm1YHn3+qExdNNcFUA03s1wJPuCvRXHIV941a0CdGgeOZbKNh9GP06xGDL9PnoOaIZQpsl4ve3lmP0l3dDndISH3/0EK6/9i4cfHiOaCc3tpOQSVPU6BVHSmBUyRGiVOCn7ZninuZsOYb8IguejtRh9s5slJvsSOwUi8xDhWjQPQH62AgU7jmAhL6toIsOg/nzDER064zyRbuEHFjbfijs5q+FnNad2AZu509wRDeFXe4JmFUoD0a52SOLzjI5UWJziKjRh4osImI0lR+kiNBBIUIunZZvFtGhD+aaUGxxQBsShUN5JpTbXdAYo7zy6BAoVQqUltigM2iE5Jmk0EEGNcylVricFSJqNEmiKZI0RY0uyC4T4eCD9GrYLDYYDWohMd5nMQl5tFqpEBLpcL1GRI92Oe2I0KuFJJruTYowfbI8miI603PQC0m0yyePdjsd4v5PjipN7Yqo0nKZkEeTDJokuvRrivKEFGFakjlLUaNpK0meSUotyV39Jc9ShGpR7rdCzRdVWkbRqD1ltPWPGu0vt/bl/WTO/rLjqqIuny5qtL9cuTp5tD/VyaNPF/H5fEWNPteS6Aslj5ZOw/JoJpAJmM4LwzAMw1x0Iy+1WfPi5pEXhmEYhmEuJLVcsIsAXvPCUmmGYRiGYeoVddp5WbNmDYYPHy7shmle+/fff6+0nwxwXnjhBREXQafTYeDAgThw4ECdXS/DMAzDnCvcLnetU6BSp50XshBu3749ZsyYUeV+in1AdsQUW2Hjxo3CSnjw4MGwWq0X/FoZhmEY5nyojWqTApU6XfMydOhQkaqCRl0oRgLFRRgxYoQoowBQMTExYoSGAkQxDMMwDBN4XLRrXg4fPiwiTtJUkYTRaBThuNevX1/tcTabDaWlpZUSwzAMw1ysDru1SYHKRas2oo4LQSMt/tBraV9VTJ48GS+//PJ/yq/83YG3W0Wh2z1fYuurQxFz3c149vZZaLtmOZYeKkL0nhW4bhVw2c13YGDHeDz64rdYOONetI3SIqTng3j6/lC8VpAJfVQiDo2/HQmXjUJMUijmPDMe1386G32bRuL3zwrw5ZgumD7JKvw0bjTm4V+VAjEaJco+fQ79ooKw/43XULC3AP1HNMOmqSuQUe7A6M/vwpRRn2HS+09BFtcYu579E4rB45DvUsLkdCNVHi98OwxKGX7fm4d4rRLhagW+25SOEUYtwvQqPL/1OK5pGo4ZO3NgNduR0i8Z2WnpsJeXIKl/GxQv2YnE0V2gDIuCeVUqjN2vgDIiFrapX0PV+g44rRvEc3LEt/F5ghSqwiCTK3DcKofF4RC+NkeKbSixOoWny+48E0x2p/B02ZtvgtoQJrxd9maVCa8cqn8gx5Pfn1MGk9UJrTEMmQXlcNrdCDJoYCq2Qh+ihVIlh6nEAl2w2uP5kl+OsBgD8jNL4XK6Ed0gBLnpxXA5nQgN1uBQuVl4qUSHJGOXuQQRBo3wa3FaPd4u5PnisJqEnwttqS75uJCfC91fuEEt/FqMOvJ2kYl8sFopvDrI08WgUYot1TWoPZ4v5OMiPF+8z4fy1G6QSu7xVXG5oFGc8G6ha5CepUahOJH3lquVMsi9biNKucdrhcql40/2fPHPS34m9HdG1yyOk3m8YihfnUeL5JlC3i/+bVXlHVLVcSfnq/SH8T/uP/+JlevW1NvlTNqo1N4prudMkXxcLpSfiz/s7XJpUuGqEKk2xwcqF+3IS02ZNGkSSkpKfCkjI6OuL4lhGIZhmEAYeYmNjRXbnJwcoTaSoNcdOnSo9jiNRiMSwzAMw1zMuN21Uwy5A3jB7kU78tKwYUPRgVm+fLmvjNavkOqoR48edXptDMMwDFNbKMRJbVOgUqcjLyaTCWlpaZUW6W7fvh3h4eFISkrChAkT8Nprr6Fp06aiM/P8888LT5iRI0fW5WUzDMMwTK2hpW1uec07IO6aB6Su99Rp5+Wff/5Bv379fK8nTpwotqNHj8bXX3+NJ598UnjB3HPPPSguLkbv3r2xaNEiaLXaOrxqhmEYhmECtvPSt29f4edSHaQ4eOWVV0RiGIZhmEsJIXeW1yIwoytw17xctAt2zzX/LvgNKSuWw3bbFPw64En8vDgdfcbehS6PLUDxkZ3Y8O1jaDf0MZQtexXY+zemFOUg+aeXkLo2FY36PIhFgx9Gt4kfoEezKHx97Zf4YMflaButx+TnrfhwcAMoslKxX6NE8vov0SlUiwSDGnsmPYPhvRogvEkEVr+5BFc8PQjfv7gARQ4Xnv/kJXz+/d2wuCogG/4wMq0fIatxf+SZnVDLZZh/2IyjxRakBKnw6bqjaB2sQZRGgemrD+P5xBAERejw4bYsPN2rAYKiDcjcewiNh7RC3o49cFpMSB7XA6Uz9sPlsCO832BYfpyHoO53wa0zwmndDDTvAbvWKGS1ZWGNxTMiWXSGRQ65Ui3yBwqsUOoM2JNXLiTRmuAw7MopQ6nNCY0xEruzSlFmdUIXFovdx0vFlqTSqVml0IbFCslzeo4J+jAj8vJJHu2CIVSLskILXC63yJNUOjhcJ6TSueklCI3Si+OyjxQhuVkEMvaZxTVGhiRgX1mxyEeHaMU9krw5KlgrpNDRwR6ptMiHePIum8UjifbKo8OC1HDZLcKVkmTRJIU2apSQC6m0Q0iiJdkxyaBdTrt4LpI8WpJF09ZT7pE8034hlZbyXqmzkFJ7x3VJFi1BsmgqJ4m2JG2mY/xl0xJU7st7iyVZtOc4fym1f13/Nv7bnr/kVnEG8uiqpMlCYi3VPUtJs9R2dRLsM2njdJwLVfGFlkX7n45l0QEila7FtFEFS6UZhmEYhmHqBwEz8sIwDMMwFxNuV0UtF+xWIFDhzgvDMAzD1AG85qXm8LQRwzAMwzD1Ch55YRiGYZg6wF1RAXctjObcp1DrXupw54VhGIZh6gJSG8lq0QFxBW7nhaeNGIZhGIapVwTMyMvEFx5C1zEzsPDzR9DvpueF34flu1Ew/N9K6MJi4H7qDsR3HoU1vYfjaFYZbv9kNj4YdT0K7S78ntUP098vxaL7ukKVsw/Py2QYmDEfZYt2YXCMHoceuxsFewtww3XNsfyBmbj6sX4Ibd4QU0Z9hklrpkIW1xjvf3EVho97CakTfxPXsyeii+g0G1VyfL09G/FaJT78+yiOFpgxIkyH95fsh7nUhiktI/Hg30dxZ88E6GP0OLz9AJpf2xZBUaHIWbIVTUf3hjIsCsWvpiJmwjCYFi8V/h+ang/D9uY0cS5X895wO3+BOa4tzA638HA55g6GpcQFhVqHveTnojVAoVJjR3YZNMHhkCtV2JpVAm1IJLYeK4bJ6+eyJZ3yDgRFJGBHhqc8KCIG+4+XwBAZCYVCjrxcM0LCg4R3S0lBufBzKS20wO10IyzGgPzMUricbkTEBot8QqMw6NQKHN19DDFh8cKjJbWsEHGhjbHdXCLuJ9aog9NqEt4r5OliLy8Rfivk6UJ+LlH+3i56tfBQIR+YCOHtYhVtGLUebxfKh2gob4eBfF5kMtGuQa0UnijkjUOeLyf8XBTCK0bKS94tGoUnr1XKff4w5Odysl+L5OkiHUd5yYtF8vKQyjzl//VzEeV+bUh1qvNzqeyp8t9fK/Iz8Fqp1Ibf/1JV3i5n4sVS1flq4+dS1XXIaujj4u/nciG8XdjPhSEoKKNbVovAjK7AXbAbMJ0XhmEYhrnoTOpqMW1UEcDTRtx5YRiGYZg6gDsvNYfXvDAMwzAMU6/gkReGYRiGqQN4zUvN4c4LwzAMw9QBFRUVqKiFz0tFAPu88LQRwzAMwzD1ioAZeRm35wvMVDaD5pFbkNz9QcSmhOLD7vdgwg+/oVejCLzXaiZW5Q/B5MjHoZbL8HG7Mrwqk6G9UYvgT57A8AYhOHDX9cjfW4DRo9tj/v+m4rjFiXu+fxhv3TgNJQ4X3v55GT6MG4ABj09DXrkTmdaP8G9sHxwpKodRpcCMbflICVKJ/DPz9mB0ZBCCjRo8PS8V73eIwbhlabCUmfHKtS1wYFOqkAO3vbM3jv++Ea3GDYQ8LBpFL+5Ag8dvgjw0CuU/fA5tv8fh1gbDad0Md/shcDv/FPebbWgkJNGU9pVCSKK3Z5ejxOYUUuhNx0thspH8OQbr0osQFBkPuVKNvw8VQh+VKPIbDxbAENsQmw8XotzqhCE6AXuOFsHpcCMkMgxZmWVwOlwwRgShOM8stnKlXOQj44OhUMpx7EABUlpFI/94tpAmN20VhfTUY0J6nBiZjL0lBUiObAy1Qo4NZYVICAuCRikX994gTOeTRDcI18FuLhH3FhuqFZJokg2TbNppNQtJNMmNSRYdplP58pIk2k1SaY1SSKKJYMp7JdFCjuxyIVij8Emeg9VKnyyZyqW8v1SaJNIEXTtJe4VUWlG1PFqSTXvyJ7bS+STZtMhXIYmuThat8Pv5Qbull9UdJ8mUq5NV+x93Oil0ZTl21XLrqmTRNZVE++friySakE7DkmimysCM4MCMNYFHXhiGYRimrtRGFJyxxqnivF1bYWEhbr/9doSEhCA0NBRjx46FyWSqtv6RI0fEj5Wq0pw5c3z1qto/e/bss76+gBl5YRiGYRjmzKCOS1ZWFpYuXQqHw4ExY8bgnnvuwffff19l/cTERFHfn88++wzvvPMOhg4dWqn8q6++wpAhQ3yvqXN0tnDnhWEYhmHqauSlFtNGFedp5CU1NRWLFi3C5s2b0aVLF1H2wQcfYNiwYZgyZQri4+P/c4xCoUBsbGylst9++w033XQTDAZDpXLqrJxc92zhaSOGYRiGqas1L7VM54P169eLDobUcSEGDhwIuVyOjRs3nlEbW7Zswfbt28V008k8+OCDiIyMRNeuXTFz5swaqaZ45IVhGIZh6jGlpaWVXms0GpFqSnZ2NqKjoyuVKZVKhIeHi31nwpdffomWLVuiZ8+elcpfeeUV9O/fH0FBQViyZAkeeOABsZbm4YcfPqtr5JEXhmEYhqkDKtzuWidpvYnRaPSlyZMnoyqefvrpahfVSmnv3r2oLRaLRayNqWrU5fnnn0evXr3QsWNHPPXUU3jyySfFupizJWBGXl5/fgH+KZiCNyLex79FPaDI3IPXn3HheesCFM7eB0WyEcX334gxAxsivEkEfu5zL8a/NQKGJk3w+nVT8NLWL/F4uzGwuCowdd1qTPu8hWh3V7vbUOKYAp1Cjun7KoQUeuL8fTiab8Zd0Xo8/N1WmEutmHFZPO76aSd+HNII+uhgvLp8Oz6+ryd0UaE4/Pvf6DxxBI5/8JeQ7zb+cCwK75vniYR83eMo//ItKAe9AbeGJNF/w9S8H8wOzzDbYWU8LFa3kEJvzrZCpTdCoVTjr/RiIYMmyfPiA3nQRydi2YE8FJc7YIhJwdLUXBEdOji+CVam5sKY0BRyhRxb0vJhjE8Q0aEPHilGWGwoMjNK4HK5ER5jQEGWSeRJCp2b7iknKfTBf7PRspMnIvTx/Zno2DEOaqUCBzbtQ6PoRthTkicky42i2mFdSZ64t+SIINhMhWL7/+3dCXxM594H8F+Wyb7ITkQSGmstRYm19Ma11EVdtd2+L1WXt1Srqnqp3qqLosq11PJ2UXzqltbHflFeS5CrqKVKXVu1zRX7EmJJMpnzfv5PnGMSE9kwmZnf9/M5nWfOnJk5eToy/zzP+f8feZ6kQseF+6k0XvPtDESX81W38g80MsDbWB063Gql6BAfk3pdIz1a2j4mY3VoWUlaboVaVVpve+emQgd7y7HIXWHay9NIV/YxuRvpypIKbawk7XkvjVnSoq3To4Wcg1u+lOj8adN6KrT1StLWqdR59lu9tnVatL7benXooqwwrT8vf/qxrfTnwlKhi7s6dIGrTee7zd92xNWhmRZNjzNVOjU1VWUF6QoadRk+fDheeumlB75mlSpV1PUoFy5cyLPfbDarDKSiXKuybNky3Lp1C3369Cn02MTERIwbNw6ZmZnFGi1ymeCFiIioLJHquqW6YNeS+1wJXKyDl4JERESorTBNmzbFtWvX1HUrDRs2VPu2bNkCi8Wigo2iTBl17ty5SO8l18WEhIQUe5qLwQsREREZ5FoVSWUeMGAA5s2bp1KlhwwZgl69ehmZRmfOnEFSUhIWLVqkLrzVnTx5Etu3b8e6deuQ35o1a3D+/Hk0adIEPj4+Kg37gw8+wFtvvYXiYvBCRERkD1JoTivFHKPl0S3MuHjxYhWwSIAiWUbdunXDzJkzjccloDl27JiaHrIm2UMxMTFo27btfa9pMpkwe/ZsDBs2TGUYJSQkYNq0aSpIKi4GL0RERPa65qUUiytaSrGoY2Eks6iggnQiPj7eZoqzjKTIZouM5lgXpysNZhsRERGRQ+HICxERkb0q7Gqlv2DXFTF4ISIisgOZMirVtJHG4MXp9WlTGceat8LbbzTD0oRWOHcnB39ZMxpjO45HhtmCj87vwOsRLTA54ygu3jJj+//WQfQf3kFq+m0AH2HS2WhE+5gQ6uWOzp9/j9fLB8Av1Bd/nrMLc5tXgn+UPzp9sQtb+tVHy6U7VM2Sf4zrgv5L/k/VEHl6/Cv47S+bUPuT4XALCselP36MqBmjofkE4tbnI+HW6V1kfjBYnWta5VbQLCvh5u6Bw1oUPH0CsOuKJ9Izb8AnOALrTlxBeqYZ/hGVsPLoeaTfykZwTDUs+yENwRWrqZovKw6cQXBsLXh6eWPdgTSExdfAlkNnYc62ICwuFoeOSd0VCyIqheG3n68iPDoInl4euPCf66qGi6fJA6nHLxk1XNTP0OIJ7Nl6VNUoSWwUg5/3n1D7n6xYFYe37kPtijVVvZaUq+dQNaqRaq9Jv4iqUQG4c7e2i7Szb6YbdV6yb15HbIifqj9ivnMTFQJ9jHakf249F4slR7XNWbeh5eTWeTFnyv8XGO1wP5Oq6yF1XEJ9c9vyHtKWcxSBd2u7CD+Th2r7mtxV/RFpSw0Xoeq8WNV28bm7X3hZFSvR67yo2i53X0N/PH9tF1t1XPLXcNHncK12563dYqOOi3U9F+vnWV8CaL3fVj0X69ewVc8l//GF1YSx5vYQ6rnYqt3yuOq52KrdwhouRPbnMsELERFRWZKjaWorzfNdFYMXIiIiO5ACuaVZWzHHdWMXZhsRERGRY+HICxERkR1w2qjkGLwQERHZAaeNSo7BCxERkR1IqnNpRk8sHHlxfnc+WohvGz6LRv/4EMfntkKQpwcGX3oKzfwl/dkDrT45hnG1IvDMuK24dSMTy16oiU5/W6fSeve92xZ1pn6DU9NegFdYKN6YsRLtvhoFj5AIHPvzciR+PR0W32Cc6zABsVs/xuVnR6r3zOn9MW7PHaJSnn+p1QmaZQcOhzdG+h0zTP7BWH/FHxmZWfALi8aCg+cQEBUPD29ffLI7FSHxteHu6YXpyT8jLKEB5uz4GTfumBFRozE+Tf4Z5uwcRNV8CstTflXt8tWrYuf+M4iuUQUeHu44cvgCYqqVh7uHG/5z4jJiqoap1OccswW1n66IQ9+dVmnFLX//JLZ/ewhtOtbPTW3e9xOSWjwDPy8PHEn+Hk061cQPm3aplN8GcU9j2+Uzql0/7lmsunxG/Zy1ooNUKnSNCoEwubsj68ZVVI8MgKeHO7JvXUeVUD91KyoF+yLrbrtioA+yb2egQqC3Sh+WlOcKAd4qbVi1A72NlOhIf291vtrdtGk9/VlSoWVfsLdJpbDK/kBvD6MtqdB6urKvp1XblJvG7GuVBi3p0UJPm9aPlfRno22VQi2p0vqtnj5rnf6sPy5MVleX6WnTkvqspxjr6dMPSn+21c6fBq2nNxeW/mz9eEEpz9bt4qQ8F5ZJXFCa86NMf7aV5lyUNhGVTS4TvBAREZUl8mdQqaaN4LoYvBAREdnrgl3wgt2SYKo0ERERORSOvBAREdkr26iUz3dVDF6IiIjsgMFLyXHaiIiIiBwKR16IiIjsgBfslpzLBC89XpuF8yvfQvhrU3F180R4hETC77/n4JMDS1WNlp7t/oaWu77F0VZvqeMrbFuGc61fUzVars2ailtrRuFE+/FIzzQDmIy1Yc8iIysHviFRmHsuBOm3shEcWxNjUi6puixSo+WNVT+hfL1nVfv1ZT8iplFbDPvmELIzcxDf+Hf4YPlh5ORYUDmxOT5ZcxRPNG2iarSs2HQSCY3rwsPTDSkpvyKhYRV8/10qcsxm1Gkca9Road2uNrau/0HVM+nSvRlWLt2BHn9qBW9PdyyY/y2eT+qgarfM3L4b/V/ohikbd6gaJW36NsDOZRtU+3c1nsW6RafxTLXfqxotSy+noVmVUFWjZe7lNNSPCcadq+dVn9SLDsad65dU+8nIQGTeuKraNcL9kXXzOqqG+qsaLVk301VtF6ljInVc4sv5qVsRG+yDnLu1W2KCfNS5Rwd6q/s5WbcR4X+vXkuYr8mo5xLs42G0A7zu1VrR23Irz5O2n8ldDSlK29+6touNto/nvedJbRedTwH1Wmy189ZzcTPqnOj1XPK39Zov1rVf5CX0GiyeBdV8sdEu6HH3QurDFKVGi3XbVg2W4tZrKWntFls1WFivhZyBpZTTRhbXjV04bURERESOxWVGXoiIiMoSThuVHIMXIiIiO2C2UckxeCEiIrJb8FKakRe4LF7zQkRERA6FIy9ERER2wGmjknOZ4CUopjp6n6uLyFpReO5AeZizc1C5xR/QaskVmLMvoma7F9B86l7U+UMPuHu6I2nCNjTs/qJKXe46YQsSe/VA70lboVk0NOvVFcNn7FCpu23/1BGT5ySrdufebbBwUTK69XpGpSh/uWAj+v+5vWrP/nglXn+9K2b8fZk6n9GjemH8hMUqRXfSuJfxl9Gf4qMPBsDk4Y7XR8zFyI8Gw+ThhoFDZ2LC/wxF/+Wr1fMGvNkKvRd9rdr/1eg5LJ+3GFpODro/9QIWTj2OP9btpVJSZ6WdwnM1I1V74oVUJD0RjrGX09T7tYoPxe276c+JlcrhTvolJFYsp1JLM29cQYPoIPWYpDzXjgpQt6JGuB/Md1Oeq4T4qNRmERvsrdpyK6QdHehltKP8PY005zDfe+1Q39z05xAfD3Vfzq2cVTvY+15qc5BVenSgl4fRlrRoPVVa529yN9J85XGdr1UqtN72LSA9WtLNbbULS5XOkzZdQFtPhS4oldpWavP97bup0lb5wAW1H0aa88NIV2aaM1FevGC35DhtRERERA7FZUZeiIiIyhIZN7GU8vmuisELERGRHXDaqOQ4bUREREQOhSMvREREdsBso5Jj8EJERGQHnDYqOZcJXvZNfx6V2o1E+q45CG46WO2z1ZZbkb99cMq9Yw9Pn4PgTz9X7QXzuiN4xjzV/viLPgiePAtTOvbLvf/+NLyXlPuciSOPY0TLOIx/+5S6P6hRRYw8/4tq96kXhdcup+HFulHq/oCr5/BCrfDcx9IvolO1UJXCLH5fpRyy76YuPxMXZLSbxgTCfCcDjSsGGCnK9cv7G+3akb5GanP1sNzVnEVCiLdqVwnJTW2WdlxwbltSkGODvIy05IqBJqNdIeBeu/zddqT/vY9ThN+9dphVW9KjdXpatH6r+tr7XjuwgLZ1WrTetk6JzpMeXUBbT38uTkp0/nZhKc8FpkIXsiJ0UdpMVyYiV+YywQsREVFZwmmjkmPwQkREZAecNio5Bi9ERER2YCnlyIvFdWMXx0iVnj17NuLj4+Hj44PExETs2bPH3qdERETktCZMmIBmzZrBz88P5cqVK9JzNE3De++9hwoVKsDX1xdt2rTBiRMn8hxz5coVvPjiiwgKClKv279/f2Rk5C4741TBy9KlS/Hmm29izJgx2L9/P+rVq4d27drhwoUL9j41IiKi0k0blXJ7VLKystC9e3cMGjSoyM/58MMPMXPmTMybNw+7d++Gv7+/+r6+c+eOcYwELkeOHMGmTZuwdu1abN++HQMHDnS+4GXatGkYMGAA+vXrh1q1aqlOkUhw/vz59j41IiKiEsvRL9ot6YZHZ+zYsRg2bBjq1KlT5FGX6dOn491330WXLl1Qt25dLFq0CGlpaVi5cqU65ujRo9iwYQM+++wzNYvSokULzJo1C0uWLFHHOc01LxL57du3D6NGjTL2ubu7q6GoXbt22XxOZmam2nTp6bmpxDdu3ICWk4Xr16+rW2GrLbfCVvtBz3sYr8H35nvzvfnefG97v3d27nGP4WLYrFKtbATj+frPpvP29lbb43T69GmcO3dOfT/rgoODVZAi39e9evVStzJV9PTTTxvHyPHyvS4jNV27di36G2pl2JkzZ+TTo/3rX//Ks3/EiBFa48aNbT5nzJgx6jncuHHjxo1bSbfU1NRH9t12+/ZtrXz58g/lPAMCAu7bJ9+DD8sXX3yhBQcHF3pcSkqKeu+0tLQ8+7t376716NFDtSdMmKBVq1btvudGRERoc+bMKdZ5lemRl5KQURq5RkZ37do1xMXF4bffflNRIBVOovhKlSohNTVVXVRFRcN+Kz72Wcmw3x5dn8mIi4zUR0dHP7JzkeQTGamQ2YXS0jQNbvkqShY06jJy5EhMnjz5ga8nUzs1atRAWVemg5fw8HB4eHjg/PnzefbL/fLly9t8TkHDZRK48B958Uh/sc+Kj/1WfOyzkmG/PZo+exx/6EoAI9vjNHz4cLz00ksPPKZKlSolem39O1m+nyXbSCf3n3rqKeOY/Mk2ZrNZZSAV9J3ukMGLl5cXGjZsiM2bN+P5559X+ywWi7o/ZMgQe58eERGRw4iIiFDbo1C5cmUVgMj3sx6syGiXXMuiZyw1bdpUzYbItazy3S62bNmivtfl2hinyjaSKaBPP/0UCxcuVMNZ0gk3b95U2UdERET08MmlFgcPHlS3OTk5qi2bdU0WmV5asWKFasvU1RtvvIHx48dj9erV+PHHH9GnTx81/aYPPtSsWRPt27dXGcRSry0lJUUNRMjFvMWdpivTIy+iZ8+euHjxoip8I1cyS0QnqVZRUbmLGBZGppCkRszjvvLakbHPSob9Vnzss5JhvxUf+6x45DtXBg109evXV7dbt25F69atVfvYsWNGRq94++231eCC1G2RERZJhZbva+vpscWLF6uAJSkpSWUZdevWTdWGKS43uWq32M8iIiIispMyP21EREREZI3BCxERETkUBi9ERETkUBi8EBERkUNx6uBl9uzZiI+PV1c6Sw65pGa5Mlm9s1OnTiolTdLa9MWy7LGcuaOYOHEiGjVqhMDAQERGRqqUP7nC3pqsmPrqq68iLCwMAQEB6ur5/IUVJd2wY8eOalFReZ0RI0ao4kzOaO7cuWpRNr0YmNR2WL9+vfE4+6twkyZNMlJPdey3+73//vuqn6w36+qw7DMnpjmpJUuWaF5eXtr8+fO1I0eOaAMGDNDKlSunnT9/XnNV69at00aPHq0tX75crUGxYsWKPI9PmjRJrWGxcuVK7YcfftA6d+6sVa5cWa3DoWvfvr1Wr1497bvvvtN27NihJSQkaL1799acVbt27dTaHocPH9YOHjyoPffcc1psbKyWkZFhHPPKK69olSpV0jZv3qx9//33WpMmTbRmzZoZj5vNZq127dpamzZttAMHDqj/D+Hh4dqoUaM0Z7R69Wrtn//8p3b8+HHt2LFj2jvvvKOZTCbVh4L99WB79uzR4uPjtbp162pDhw419rPf7idr+Dz55JPa2bNnje3ixYvG4+wz5+W0wYss3Pjqq68a93NycrTo6Ght4sSJdj2vsiJ/8GKxWNRCYVOmTDH2Xbt2TfP29ta++uordf+nn35Sz9u7d69xzPr16zU3Nze1iKYruHDhguqD5ORko4/ki/mbb74xjjl69Kg6ZteuXeq+/EJ0d3fXzp07Zxwzd+5cLSgoSMvMzNRcQUhIiPbZZ5+xvwpx48YNrWrVqtqmTZu0Vq1aGcEL+63g4EX+mLKFfebcnHLaSBa7kvLD1ktzSzEcuS9LclPxlzMXhS1n7gr0gkyhoaHqVj5n2dnZefpNhq1jY2Pz9FudOnXyFFZs166dKp195MgRODOpzLlkyRJVuEqmj9hfDyZTHDKFYd0/gv1WMJnalqlwWZNHprRlGkiwz5xbma+wWxKXLl1SvzTzV+GV+//+97/tdl5lmQQuwlaf6Y/JrcwJW/P09FRf5PoxzkzW35BrEJo3b47atWurffJzyxpcEtQ9qN9s9av+mDOS0uASrMg1B3KtgZQQr1Wrliovzv6yTYK8/fv3Y+/evfc9xs+ZbfLH1YIFC1C9enWcPXsWY8eORcuWLXH48GH2mZNzyuCF6FH9VSy/FHfu3GnvUynz5MtEAhUZqVq2bBn69u2L5ORke59WmZWamoqhQ4di06ZNj32lYUfWoUMHoy0XiUswExcXh6+//lolHZDzcsppo/DwcHh4eNx3VbncL+6y267CejnzgvrsYS5n7mhkLY61a9eqdT1iYmKM/fJzyzSlrOPxoH6z1a/6Y85I/uJNSEhQK8dKxla9evUwY8YM9lcBZIpD/m01aNBAjWbKJsGerPkibRkNYL8VTkZZqlWrhpMnT/Kz5uTcnfUXp/zSlKW5rYf85b4MZdODlzPX6cuZ631mvZy5rqTLmTsKubZZAheZ9pCfVfrJmnzOTCZTnn6TVGqZd7fuN5lGsQ785C9sSSOWqRRXIJ+RzMxM9lcBZJE6+Zn1lXtlk2vL5BoOvc1+K5yUbTh16pQq98DPmpPTnDhVWjJlFixYoLJkBg4cqFKlra8qdzWSySDpgLLJ//pp06ap9q+//mqkSksfrVq1Sjt06JDWpUsXm6nS9evX13bv3q3t3LlTZUY4c6r0oEGDVPr4tm3b8qRj3rp1K086pqRPb9myRaVjNm3aVG350zHbtm2r0q03bNigRUREOG065siRI1U21unTp9XnSO5LRtrGjRvV4+yvorHONhLst/sNHz5c/duUz1pKSopKeZZUZ8kKFOwz5+W0wYuYNWuW+uBKvRdJnZbaJK5s69atKmjJv/Xt29dIl/7rX/+qRUVFqcAvKSlJ1emwdvnyZRWsBAQEqHTCfv36qaDIWdnqL9mk9otOgrvBgwerdGA/Pz+ta9euKsCx9ssvv2gdOnTQfH191S9X+aWbnZ2tOaOXX35Zi4uLU//u5ItAPkd64CLYXyULXthv9+vZs6dWoUIF9VmrWLGiun/y5EnjcfaZ83KT/9h79IeIiIjIpa95ISIiIufF4IWIiIgcCoMXIiIicigMXoiIiMihMHghIiIih8LghYiIiBwKgxciIiJyKAxeiAitW7dWK2YTETkCBi9ERETkUBi8EBERkUNh8ELkYm7evIk+ffogICBArb47depUe58SEVGxMHghcjEjRoxAcnIyVq1ahY0bN2Lbtm3Yv3+/vU+LiKjIPIt+KBE5uoyMDHz++ef48ssvkZSUpPYtXLgQMTEx9j41IqIi48gLkQs5deoUsrKykJiYaOwLDQ1F9erV7XpeRETFweCFiIiIHAqDFyIX8sQTT8BkMmH37t3GvqtXr+L48eN2PS8iouLgNS9ELkQyjPr3768u2g0LC0NkZCRGjx4Nd3f+HUNEjoPBC5GLmTJlirpwt1OnTggMDMTw4cORnp5u79MiIioyN03TtKIfTkRERGRfHCsmIiIih8LghYiIiBwKgxciIiJyKAxeiIiIyKEweCEiIiKHwuCFiIiIHAqDFyIiInIoDF6IiIjIoTB4ISIiIofC4IWIiIgcCoMXIiIicigMXoiIiAiO5P8BT59CqFrYKR8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualize positional encodings\n",
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('d')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fE4A8Lv_4IF"
      },
      "source": [
        "Each row in the positional encoding matrix corresponds to a specific position in the input sequence. Since each position receives a unique encoding, the model can distinguish between the order of tokens, even when processing the sequence in parallel.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Masking Strategies in Transformers\n",
        "\n",
        "When building a Transformer model, certain tokens should be ignored during attention calculations to prevent misleading results. Two common masking strategies are used:\n",
        "\n",
        "- **Padding Mask**: Ignores padding tokens added for sequence alignment.\n",
        "- **Look-Ahead Mask**: Prevents the model from attending to future tokens during decoding (used in autoregressive tasks like translation).\n",
        "\n",
        "---\n",
        "\n",
        "### Padding Mask\n",
        "\n",
        "In real-world NLP projects, input sequences will likely vary in length. However, Transformer models expect inputs of a fixed size. To align these sequences, shorter sequences are padded with zeros, and longer ones may be truncated:\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Original tokenized input:\n",
        "```plaintext\n",
        "[[ 71, 121,   4,  56,  99, 2344,  345, 1284,   15],\n",
        " [ 56, 1285,  15, 181, 545],\n",
        " [ 87,  600]]\n",
        "```\n",
        "\n",
        "After padding to a fixed length of 5:\n",
        "```plaintext\n",
        "[[ 71, 121,   4,  56,  99],\n",
        " [2344,  345, 1284,   15,    0],\n",
        " [ 56, 1285,   15, 181,  545],\n",
        " [ 87,  600,    0,   0,    0]]\n",
        "```\n",
        "\n",
        "The padding values (`0`) are not meaningful inputs. If they were included in attention calculations, they would skew the output. Thats why we use a **padding mask** to assign very negative scores (like `-1e9`) to these positions before applying the softmax. This ensures they contribute essentially nothing to the attention output.\n",
        "\n",
        "**Conceptually:**\n",
        "- Input: `[87, 600, 0, 0, 0]`\n",
        "- Masked: `[87, 600, -1e9, -1e9, -1e9]`\n",
        "- After softmax, the masked values will have zero weight.\n",
        "\n",
        "This approach is supported by many Transformer implementations, such as Keras’s [`MultiHeadAttention`](https://keras.io/api/layers/attention_layers/multi_head_attention/) layer, which accepts padding masks as input.\n",
        "\n",
        "> Note: The function below generates a mask for an already padded sequence.\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "JOL9XWsFQxxo"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(decoder_token_ids):\n",
        "    \"\"\"\n",
        "    Creates a matrix mask for the padding cells\n",
        "    Arguments:\n",
        "        decoder_token_ids -- (n, m) matrix\n",
        "    Returns:\n",
        "        mask -- (n, 1, m) binary tensor\n",
        "    \"\"\"\n",
        "    seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    # this will allow for broadcasting later when comparing sequences\n",
        "    return seq[:, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J5FFjklQ1Fz",
        "outputId": "829f523e-7f76-4b15-c6a5-6f23b0e9d5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[1. 1. 0. 0. 1.]]\n",
            "\n",
            " [[1. 1. 1. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 1.]]], shape=(3, 1, 5), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n",
        "print(create_padding_mask(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZavd87r_4IF",
        "outputId": "1a4dec7a-b901-4649-b4b5-f2f1cf0cc84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[7.2876638e-01 2.6809821e-01 6.6454895e-04 6.6454895e-04 1.8064313e-03]\n",
            " [8.4437370e-02 2.2952458e-01 6.2391245e-01 3.1062771e-02 3.1062771e-02]\n",
            " [4.8541022e-03 4.8541022e-03 4.8541022e-03 2.6502505e-01 7.2041267e-01]], shape=(3, 5), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[7.2973621e-01 2.6845497e-01 0.0000000e+00 0.0000000e+00 1.8088353e-03]\n",
            "  [2.4472848e-01 6.6524088e-01 0.0000000e+00 0.0000000e+00 9.0030566e-02]\n",
            "  [6.6483538e-03 6.6483538e-03 0.0000000e+00 0.0000000e+00 9.8670328e-01]]\n",
            "\n",
            " [[7.3057157e-01 2.6876229e-01 6.6619506e-04 0.0000000e+00 0.0000000e+00]\n",
            "  [9.0030566e-02 2.4472848e-01 6.6524088e-01 0.0000000e+00 0.0000000e+00]\n",
            "  [3.3333334e-01 3.3333334e-01 3.3333334e-01 0.0000000e+00 0.0000000e+00]]\n",
            "\n",
            " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01 7.3105854e-01]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.0000000e-01 5.0000000e-01]\n",
            "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01 7.3105854e-01]]], shape=(3, 3, 5), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(tf.keras.activations.softmax(x))\n",
        "print(tf.keras.activations.softmax(x + (1 - create_padding_mask(x)) * -1.0e9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waT6_PRx_4IF"
      },
      "source": [
        "### Look-Ahead Mask (Causal Masking)\n",
        "\n",
        "In autoregressive tasks like language modeling or text generation, the model should not have access to future tokens when predicting the current one. This is where a **look-ahead mask** (AKA causal mask) is applied.\n",
        "\n",
        "The look-ahead mask makes sure each position in the decoder can only attend to previous positions, not future ones.\n",
        "\n",
        "---\n",
        "\n",
        "**Conceptual Example:**\n",
        "\n",
        "Suppose the true output sequence is:\n",
        "\n",
        "[1, 2, 3]\n",
        "\n",
        "\n",
        "During training, you don’t want the model to peek at positions 2 and 3 while predicting position 1. So, you apply a mask that hides future values at each time step:\n",
        "\n",
        "| Step | Visible Input        | Masked Output         |\n",
        "|------|----------------------|------------------------|\n",
        "| 1    | `[1, -1e9, -1e9]`    | Predict token 1        |\n",
        "| 2    | `[1, 2, -1e9]`       | Predict token 2        |\n",
        "| 3    | `[1, 2, 3]`          | Predict token 3        |\n",
        "\n",
        "This allows the model to simulate sequential prediction while still training in parallel.\n",
        "\n",
        "Most modern Transformer libraries support this natively through a look-ahead or causal mask input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "9O9UbM31Q3hK"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(sequence_length):\n",
        "    \"\"\"\n",
        "    Returns a lower triangular matrix filled with ones\n",
        "    Arguments:\n",
        "        sequence_length -- matrix size\n",
        "    Returns:\n",
        "        mask -- (size, size) tensor\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(SEED)\n",
        "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfzHoVj9Q5nG",
        "outputId": "0565b9c1-659b-4db9-f199-12ed70862c67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\n",
              "array([[[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]]], dtype=float32)>"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0gPyv0oDBi"
      },
      "source": [
        "## 3. Self-Attention\n",
        "\n",
        "Self-attention is a very important component of the Transformer architecture. It allows the model to weigh the relevance of every other token in the sequence when processing a particular token. Unlike traditional RNNs, self-attention is fully parallelizable, making it efficient for large-scale training.\n",
        "\n",
        "<img src=\"../assets/self-attention.png\" alt=\"Self-Attention Illustration\" width=\"600\"/>\n",
        "\n",
        "In the Transformer model, **scaled dot-product attention** is used to calculate attention scores between tokens. Each token is projected into three different spaces to produce:\n",
        "\n",
        "- **Queries (Q)**\n",
        "- **Keys (K)**\n",
        "- **Values (V)**\n",
        "\n",
        "These are combined using the following equation:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_k}} + M\\right) V \\tag{4}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $Q$ is the matrix of queries\n",
        "- $K$ is the matrix of keys\n",
        "- $V$ is the matrix of values\n",
        "- $M$ is an optional mask (ex: padding or look-ahead)\n",
        "- $d_k$ is the dimensionality of the keys and is used for scaling\n",
        "\n",
        "Scaling by $\\sqrt{d_k}$ prevents the dot products from growing too large, which helps stabilize gradients when the softmax is applied.\n",
        "\n",
        "---\n",
        "\n",
        "### Implementing Scaled Dot-Product Attention\n",
        "\n",
        "The implementation of scaled dot-product attention takes in query, key, value, and an optional mask. The mask is typically used to prevent attention to certain positions (ex: padding tokens or future tokens in the decoder).\n",
        "\n",
        "When a mask is provided, we modify the attention logits by applying:\n",
        "\n",
        "```python\n",
        "masked_logits = logits + (1.0 - mask) * -1e9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "CSysk_rjQ7lp"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Calculate the attention weights.\n",
        "      q, k, v must have matching leading dimensions.\n",
        "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "      The mask has different shapes depending on its type(padding or look ahead)\n",
        "      but it must be broadcastable for addition.\n",
        "    Arguments:\n",
        "        q -- query shape == (..., seq_len_q, depth)\n",
        "        k -- key shape == (..., seq_len_k, depth)\n",
        "        v -- value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable\n",
        "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    Returns:\n",
        "        output -- attention_weights\n",
        "    \"\"\"\n",
        "    tf.random.set_seed(SEED)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None: # Don't replace this None\n",
        "        scaled_attention_logits += (1.0 - mask)*-1e9\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxVbTBd-_4IG",
        "outputId": "4de0bf20-25f5-4779-8629-71cf62b5cc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92mAll tests passed\n"
          ]
        }
      ],
      "source": [
        "# UNIT TEST\n",
        "scaled_dot_product_attention_test(scaled_dot_product_attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blS0pEpTqRVI"
      },
      "source": [
        "With self-attention implemented, the next step is to build the Transformer encoder block.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Encoder Architecture\n",
        "\n",
        "The Transformer encoder is composed of two main sub-layers:\n",
        "\n",
        "1. **Multi-Head Self-Attention**  \n",
        "2. **Feed-Forward Neural Network**\n",
        "\n",
        "These components are stacked together with residual connections and layer normalization to create a powerful and scalable architecture. The encoder generates context-aware representations of the input tokens and passes key and value vectors to the decoder.\n",
        "\n",
        "<img src=\"../assets/encoder_layer.png\" alt=\"Transformer Encoder Layer\" width=\"400\"/>\n",
        "\n",
        "In this setup:\n",
        "- The **Multi-Head Attention** layer allows the model to jointly attend to information from different representation subspaces.\n",
        "- The **Feed-Forward Network (FFN)** consists of two dense layers, applied independently to each position in the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### Encoder Block Design\n",
        "\n",
        "1. **Multi-Head Self-Attention**  \n",
        "   - Allows the encoder to look at all positions in the input sequence when processing each token.\n",
        "   - Allows learning multiple types of relationships by using multiple attention heads.\n",
        "   - Implemented using [`tf.keras.layers.MultiHeadAttention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)\n",
        "\n",
        "2. **Feed-Forward Network**  \n",
        "   - A small two-layer neural network applied identically to each position.\n",
        "   - Typically includes a ReLU or GELU activation between the dense layers.\n",
        "   - Implemented using the [Keras Sequential API](https://keras.io/api/models/sequential/)\n",
        "\n",
        "The attention output is first passed through a dropout and normalization layer before being added back to the original input (residual connection). The same pattern is repeated after the FFN. This structure is repeated in multiple encoder layers to form the full Transformer encoder stack.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "sC5vJhz29vZR"
      },
      "outputs": [],
      "source": [
        "def FullyConnected(embedding_dim, fully_connected_dim):\n",
        "    tf.random.set_seed(SEED)\n",
        "    return tf.keras.Sequential([tf.keras.layers.Dense(fully_connected_dim, activation='relu'), # (batch_size, seq_len, dff)\n",
        "                                tf.keras.layers.Dense(embedding_dim)]) # (batch_size, seq_len, embedding_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R65WbX5wqYYH"
      },
      "source": [
        "### 4.1. Building the Encoder Layer\n",
        "\n",
        "With the core components in place, I can now make a full encoder layer by combining:\n",
        "\n",
        "- Multi-head self-attention\n",
        "- Feed-forward neural network (FFN)\n",
        "- Residual connections\n",
        "- Layer normalization\n",
        "- Dropout for regularization\n",
        "\n",
        "This structure mirrors the standard encoder block design introduced in the original Transformer paper. A diagram of the layer structure is shown below:\n",
        "\n",
        "<img src=\"../assets/encoder_layer.png\" alt=\"Encoder Layer Diagram\" width=\"400\"/>\n",
        "\n",
        "---\n",
        "\n",
        "### Encoder Layer Implementation Overview\n",
        "\n",
        "The encoder layer operates as follows:\n",
        "\n",
        "1. **Self-Attention**  \n",
        "   The input is passed through a multi-head self-attention mechanism. For self-attention, the query, key, and value inputs are all the same (`Q = K = V`). An optional mask can be applied here to ignore specific tokens (e.g., padding).\n",
        "\n",
        "2. **Residual Connection + Normalization**  \n",
        "   A skip connection is added between the input and the attention output. This sum is then passed through a layer normalization layer.\n",
        "\n",
        "3. **Feed-Forward Network**  \n",
        "   The output is then passed through a small two-layer feed-forward neural network. Dropout is applied for regularization.\n",
        "\n",
        "4. **Second Residual Connection + Normalization**  \n",
        "   Another skip connection is applied between the FFN input and its output, followed by a second layer normalization.\n",
        "\n",
        "This design allows each encoder block to build increasingly rich representations of the input, while the residual connections help with gradient flow during training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "tIufbrc-9_2u"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.layers import LayerNormalization, Dropout, MultiHeadAttention, Dense\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim,\n",
        "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        tf.random.set_seed(SEED)  # Set seed before defining layers\n",
        "\n",
        "        # Fix the initializer for MultiHeadAttention\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embedding_dim,\n",
        "                                      dropout=dropout_rate,\n",
        "                                      kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED))\n",
        "\n",
        "        # Fix the initializer for FullyConnected layers\n",
        "        self.ffn = tf.keras.Sequential([Dense(fully_connected_dim, activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED)),\n",
        "                                        Dense(embedding_dim, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED))])\n",
        "\n",
        "        # Fix the initializer for LayerNormalization\n",
        "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        # Fix the Dropout seed\n",
        "        self.dropout_ffn = Dropout(dropout_rate, seed=SEED)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        tf.random.set_seed(SEED)  # Ensure deterministic behavior at every call\n",
        "\n",
        "        self_mha_output = self.mha(query=x, value=x, key=x, attention_mask=mask, return_attention_scores=False, training=training)\n",
        "\n",
        "        # Skip connection + LayerNorm\n",
        "        skip_x_attention = self.layernorm1(x + self_mha_output)\n",
        "\n",
        "        # Pass through feedforward\n",
        "        ffn_output = self.ffn(skip_x_attention)\n",
        "\n",
        "        # Apply dropout (only during training)\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "\n",
        "        # Final layer norm\n",
        "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)\n",
        "        print(encoder_layer_out)\n",
        "\n",
        "        return encoder_layer_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNfcWkuN_4IG",
        "outputId": "aa076c9e-635a-4fe7-84fd-7658ea34abfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"layer_normalization_34_1/add_2:0\", shape=(1, 3, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[ 0.9754241   0.15179627  0.52959394 -1.6568143 ]\n",
            "  [-0.24592474  0.44221142  1.266537   -1.4628237 ]\n",
            "  [ 1.6900113  -0.35137552 -0.41989863 -0.9187371 ]]], shape=(1, 3, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[ 0.7332504   0.01539418  0.8870007  -1.6356452 ]\n",
            "  [-0.4855674   0.33046782  1.4271767  -1.2720772 ]\n",
            "  [ 1.6128805  -0.39568523 -0.10796562 -1.1092294 ]]], shape=(1, 3, 4), dtype=float32)\n",
            "\u001b[92mAll tests passed\n"
          ]
        }
      ],
      "source": [
        "# UNIT TEST\n",
        "tf.random.set_seed(SEED)\n",
        "EncoderLayer_test(EncoderLayer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHYSVtcE_4IG"
      },
      "source": [
        "### Building the Full Transformer Encoder\n",
        "\n",
        "Now, the key components of the Transformer encoder are implemented: positional encodings, scaled dot-product self-attention, and a single encoder layer. Next, I bring everything together to build the **full encoder block** which is the backbone of the Transformer architecture.\n",
        "\n",
        "The full encoder consists of:\n",
        "\n",
        "- An embedding layer to convert token indices into dense vectors\n",
        "- Positional encodings added to the embeddings to incorporate order\n",
        "- A stack of multiple encoder layers\n",
        "- Dropout and scaling applied at key steps to improve regularization and training stability\n",
        "\n",
        "<img src=\"../assets/encoder.png\" alt=\"Full Transformer Encoder\" width=\"330\"/>\n",
        "\n",
        "---\n",
        "\n",
        "### Encoder Class Overview\n",
        "\n",
        "The encoder module follows this sequence of operations:\n",
        "\n",
        "1. **Embedding**  \n",
        "   The input token indices are passed through an `Embedding` layer to produce dense vector representations.\n",
        "\n",
        "2. **Embedding Scaling**  \n",
        "   The output of the embedding layer is scaled by $\\sqrt{d_{\\text{model}}}$ to prevent small initialization values from slowing down training. Be sure to cast `d_model` to `tf.float32` before the square root.\n",
        "\n",
        "3. **Positional Encoding**  \n",
        "   Positional information is added by summing the embeddings with a precomputed positional encoding matrix:  \n",
        "   ```python\n",
        "   x += self.pos_encoding[:, :seq_len, :]\n",
        "   ```\n",
        "\n",
        "4. **Dropout**  \n",
        "   A `Dropout` layer is applied after the positional encoding for regularization during training. \n",
        "\n",
        "5. **Encoder Layer Stack**  \n",
        "   The result is passed through a sequence of encoder layers (already defined) using a for loop. Each encoder layer is responsible for refining the input representations via multi-head attention and a feed forward network.\n",
        "\n",
        "This stacked architecture makes the encoder capture increasingly abstract relationships between tokens across the input sequence.\n",
        "\n",
        "The encoder can be reused in a variety of tasks, including machine translation, classification, and language modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "7j2Tjr0K0t0I"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding, Dropout\n",
        "import tensorflow as tf\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    encoder Layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "                 maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Encoder, self).__init__()\n",
        "        tf.random.set_seed(SEED)  # Set seed before defining layers\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Use a fixed initializer with the seed for reproducibility\n",
        "        self.embedding = Embedding(input_vocab_size, self.embedding_dim,\n",
        "                                   embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=SEED))\n",
        "\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        # Create EncoderLayer instances (each with its own reproducibility settings)\n",
        "        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        # Initialize dropout with a fixed seed\n",
        "        self.dropout = Dropout(dropout_rate, seed=SEED)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # Do not reset the seed here; rely on the seed set once during initialization.\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Pass input through the Embedding layer and scale it\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "        # Add positional encoding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        # Apply dropout (only during training)\n",
        "        x = self.dropout(x, training=training)\n",
        "        # Pass through the stack of encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "        return x  # (batch_size, input_seq_len, embedding_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nU90yXs_4IG"
      },
      "source": [
        "## 5. Transformer Decoder\n",
        "\n",
        "The decoder is responsible for generating the output sequence, one token at a time, during autoregressive tasks such as translation or text generation. It processes the output sequence (shifted right during training) while attending to the encoder's outputs.\n",
        "\n",
        "The decoder layer incorporates two multi-head attention blocks:\n",
        "\n",
        "1. A self-attention layer that uses a look-ahead mask to prevent attending to future positions.\n",
        "2. A cross-attention layer that attends to the encoder’s key and value outputs using the current decoder state as the query.\n",
        "\n",
        "<img src=\"../assets/decoder_layer.png\" alt=\"Transformer Decoder Layer\" width=\"250\"/>\n",
        "\n",
        "---\n",
        "\n",
        "### Decoder Layer Design\n",
        "\n",
        "Each decoder layer follows a similar structure to the encoder but includes an additional self-attention block at the beginning.\n",
        "\n",
        "The layer consists of three sub-blocks:\n",
        "\n",
        "#### **Block 1: Masked Self-Attention**\n",
        "- Applies a look-ahead mask so that each position can only attend to previous positions in the output sequence.\n",
        "- This simulates autoregressive generation during training.\n",
        "- A residual connection and layer normalization follow the attention output.\n",
        "\n",
        "#### **Block 2: Encoder-Decoder Cross-Attention**\n",
        "- Calculates attention between the decoder’s current representation and the encoder's output.\n",
        "- The query comes from the decoder, while keys and values come from the encoder.\n",
        "- This allows the decoder to align its output generation with the encoded input.\n",
        "- A second residual connection and normalization are applied.\n",
        "\n",
        "#### **Block 3: Feed-Forward Network**\n",
        "- Similar to the encoder, a two-layer dense network is applied independently to each position.\n",
        "- Dropout is included for regularization, and another residual connection with layer normalization is applied.\n",
        "\n",
        "Each decoder layer allows the model to combine knowledge from both the input and previously generated tokens, allowing for flexible and powerful sequence generation. Multiple decoder layers are stacked to form the full Transformer decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "wEouNFvCzMeT"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The decoder layer is composed by two multi-head attention blocks,\n",
        "    one that takes the new input and uses self-attention, and the other\n",
        "    one that combines it with the output of the encoder, followed by a\n",
        "    fully connected block.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        tf.random.set_seed(SEED)  # Set seed before defining layers\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embedding_dim,\n",
        "                                      dropout=dropout_rate,\n",
        "                                      kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED))\n",
        "\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embedding_dim,\n",
        "                                      dropout=dropout_rate,\n",
        "                                      kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED))\n",
        "        for layer in self.mha1.trainable_variables:\n",
        "          layer.assign(tf.keras.initializers.GlorotUniform(seed=SEED)(shape=layer.shape))\n",
        "\n",
        "        for layer in self.mha2.trainable_variables:\n",
        "          layer.assign(tf.keras.initializers.GlorotUniform(seed=SEED)(shape=layer.shape))\n",
        "\n",
        "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
        "                                  fully_connected_dim=fully_connected_dim,\n",
        "                                 )\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = Dropout(dropout_rate, seed=SEED)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder Layer\n",
        "        Arguments:\n",
        "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask -- Boolean mask for the target_input\n",
        "            padding_mask -- Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "            attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "        # enc_output.shape == (batch_size, input_seq_len, embedding_dim)\n",
        "\n",
        "        # BLOCK 1\n",
        "        # calculate self-attention and return attention scores as attn_weights_block1.\n",
        "        # Dropout will be applied during training (~1 line).\n",
        "        mult_attn_out1, attn_weights_block1 = self.mha1(query=x,\n",
        "                                                        value=x,\n",
        "                                                        key=x,\n",
        "                                                        attention_mask=look_ahead_mask,\n",
        "                                                        return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # apply layer normalization (layernorm1) to the sum of the attention output and the input (~1 line)\n",
        "        Q1 = self.layernorm1(x + mult_attn_out1)\n",
        "\n",
        "        # BLOCK 2\n",
        "        # calculate self-attention using the Q from the first block and K and V from the encoder output.\n",
        "        # Dropout will be applied during training\n",
        "        # Return attention scores as attn_weights_block2 (~1 line)\n",
        "        mult_attn_out2, attn_weights_block2 = self.mha2(query=Q1,\n",
        "                                                        value=enc_output,\n",
        "                                                        key=enc_output,\n",
        "                                                        attention_mask=padding_mask,\n",
        "                                                        return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # apply layer normalization (layernorm2) to the sum of the attention output and the output of the first block (~1 line)\n",
        "        mult_attn_out2 = self.layernorm2(Q1 + mult_attn_out2)  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        #BLOCK 3\n",
        "        # pass the output of the second block through a ffn\n",
        "        ffn_output = self.ffn(mult_attn_out2)  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # apply a dropout layer to the ffn output\n",
        "        # use `training=training`\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "\n",
        "        # apply layer normalization (layernorm3) to the sum of the ffn output and the output of the second block\n",
        "        out3 = self.layernorm3(mult_attn_out2 + ffn_output) # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yt6oqcG_4IH",
        "outputId": "a90b9fe9-6e6a-49de-820f-db3c26e5c6f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92mAll tests passed\n"
          ]
        }
      ],
      "source": [
        "# UNIT TEST\n",
        "DecoderLayer_test(DecoderLayer, create_look_ahead_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGFdatZb_4IJ"
      },
      "source": [
        "### Building the Full Transformer Decoder\n",
        "\n",
        "The full decoder module combines all components developed so far: token embeddings, positional encodings, and a stack of decoder layers to generate output sequences autoregressively.\n",
        "\n",
        "The decoder receives the target sequence as input (typically shifted right during training), embeds it, adds positional information, and processes it through multiple decoder layers. Each decoder layer applies masked self-attention, cross-attention with encoder outputs, and a feed-forward network.\n",
        "\n",
        "<img src=\"../assets/decoder.png\" alt=\"Transformer Decoder Architecture\" width=\"300\"/>\n",
        "\n",
        "\n",
        "### Decoder Structure\n",
        "\n",
        "The decoder follows these steps:\n",
        "\n",
        "1. **Embedding Layer**  \n",
        "   Converts the token indices from the target sequence into dense vector representations using a learned embedding matrix.\n",
        "\n",
        "2. **Embedding Scaling**  \n",
        "   The embedding vectors are scaled by $\\sqrt{d_{\\text{model}}}$ to control the variance and stabilize training.\n",
        "\n",
        "3. **Positional Encoding**  \n",
        "   The positional encoding is added to the scaled embeddings to inject information about token order.\n",
        "\n",
        "4. **Dropout**  \n",
        "   Dropout is applied after the positional encoding to prevent overfitting during training.\n",
        "\n",
        "5. **Stack of Decoder Layers**  \n",
        "   The encoded input is passed through a stack of identical decoder layers. Each layer includes:\n",
        "   - Masked self-attention (with look-ahead mask)\n",
        "   - Cross-attention with encoder outputs\n",
        "   - Feed-forward network\n",
        "   - Residual connections and normalization\n",
        "\n",
        "The decoder can be used for tasks such as machine translation, text summarization, or any sequence generation problem where context from an input sequence is needed to condition the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "McS3by6k4pnP"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the target input to an embedding layer\n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    decoder Layers\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n",
        "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Decoder, self).__init__()\n",
        "        tf.random.set_seed(SEED)  # Set seed before defining layers\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
        "                                        num_heads=num_heads,\n",
        "                                        fully_connected_dim=fully_connected_dim,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        layernorm_eps=layernorm_eps)\n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate, seed=SEED)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "        \"\"\"\n",
        "        Forward  pass for the Decoder\n",
        "\n",
        "        Arguments:\n",
        "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            look_ahead_mask -- Boolean mask for the target_input\n",
        "            padding_mask -- Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n",
        "            attention_weights - Dictionary of tensors containing all the attention weights\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        # create word embeddings\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, embedding_dim)\n",
        "\n",
        "        # scale embeddings by multiplying by the square root of their dimension\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "\n",
        "        # calculate positional encodings and add to word embedding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # apply a dropout layer to x\n",
        "        # use `training=training`\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # use a for loop to pass x through a stack of decoder layers and update attention_weights (~4 lines total)\n",
        "        for i in range(self.num_layers):\n",
        "            # pass x and the encoder output through a stack of decoder layers and save the attention weights\n",
        "            # of block 1 and 2 (~1 line)\n",
        "            x, block1, block2 = self.dec_layers[i](x=x, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
        "\n",
        "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
        "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2\n",
        "        #x.shape == (batch_size, target_seq_len, embedding_dim)\n",
        "        return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3Iqg3_4_4IK",
        "outputId": "f54a39b4-ca5d-4430-dee0-91bfbc901d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92mAll tests passed\n"
          ]
        }
      ],
      "source": [
        "# # UNIT TEST\n",
        "Decoder_test(Decoder, create_look_ahead_mask, create_padding_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK9Mop1Y_4IK"
      },
      "source": [
        "## 6. Assembling the Full Transformer Model\n",
        "\n",
        "The Transformer architecture brings together the encoder and decoder components into a unified model designed for sequence-to-sequence tasks. This includes applications like machine translation, summarization, and question answering.\n",
        "\n",
        "The full model consists of stacked encoder and decoder layers, along with a final dense output layer that generates predictions.\n",
        "\n",
        "<img src=\"../assets/transformer.png\" alt=\"Full Transformer Architecture\" width=\"550\"/>\n",
        "\n",
        "### Architecture Flow\n",
        "\n",
        "1. **Input → Encoder**  \n",
        "   The source input sequence is first passed through the encoder, which consists of:\n",
        "   - Token embeddings\n",
        "   - Positional encodings\n",
        "   - Multiple stacked encoder layers (each with multi-head self-attention and feed-forward networks)\n",
        "\n",
        "2. **Target → Decoder**  \n",
        "   The target sequence (shifted during training) is passed through the decoder:\n",
        "   - Token embeddings and positional encodings are applied to the target\n",
        "   - Masked self-attention is used to prevent the decoder from attending to future tokens\n",
        "   - Cross-attention integrates information from the encoder’s output\n",
        "   - A feed-forward network further refines each decoder layer’s output\n",
        "\n",
        "3. **Output Layer**  \n",
        "   The final decoder output is passed through a linear `Dense` layer, followed by a `softmax` activation to generate token level predictions over the vocabulary.\n",
        "\n",
        "\n",
        "### Transformer Class Design\n",
        "\n",
        "The main model combines all components into a single `Transformer` class, which performs the following steps in the `call()` method:\n",
        "\n",
        "1. Pass the input sequence through the encoder using the appropriate padding mask.\n",
        "2. Pass the encoder output and target sequence through the decoder, using both the look-ahead and padding masks.\n",
        "3. Apply a final linear projection to the decoder’s output.\n",
        "4. Use a `softmax` activation to produce a probability distribution over the vocabulary for each position in the output.\n",
        "\n",
        "This modular design allows flexibility for reuse across a variety of NLP tasks and training strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "QHymPmaj-2ba"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Complete transformer with an Encoder and a Decoder\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
        "               target_vocab_size, max_positional_encoding_input,\n",
        "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               input_vocab_size=input_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_input,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=num_layers,\n",
        "                               embedding_dim=embedding_dim,\n",
        "                               num_heads=num_heads,\n",
        "                               fully_connected_dim=fully_connected_dim,\n",
        "                               target_vocab_size=target_vocab_size,\n",
        "                               maximum_position_encoding=max_positional_encoding_target,\n",
        "                               dropout_rate=dropout_rate,\n",
        "                               layernorm_eps=layernorm_eps)\n",
        "\n",
        "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, input_sentence, output_sentence, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire Transformer\n",
        "        Arguments:\n",
        "            input_sentence -- Tensor of shape (batch_size, input_seq_len)\n",
        "                              An array of the indexes of the words in the input sentence\n",
        "            output_sentence -- Tensor of shape (batch_size, target_seq_len)\n",
        "                              An array of the indexes of the words in the output sentence\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            enc_padding_mask -- Boolean mask to ensure that the padding is not\n",
        "                    treated as part of the input\n",
        "            look_ahead_mask -- Boolean mask for the target_input\n",
        "            dec_padding_mask -- Boolean mask for the second multihead attention layer\n",
        "        Returns:\n",
        "            final_output -- Describe me\n",
        "            attention_weights - Dictionary of tensors containing all the attention weights for the decoder\n",
        "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
        "        \"\"\"\n",
        "        # call self.encoder with the appropriate arguments to get the encoder output\n",
        "        enc_output = self.encoder(x=input_sentence, training=training, mask=enc_padding_mask)  # (batch_size, inp_seq_len, embedding_dim)\n",
        "\n",
        "        # call self.decoder with the appropriate arguments to get the decoder output\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, embedding_dim)\n",
        "        dec_output, attention_weights = self.decoder(x=output_sentence, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
        "\n",
        "        # pass decoder output through a linear layer and softmax (~2 lines)\n",
        "        final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5yGTF4r_4IK",
        "outputId": "63285bc0-ef5c-4b68-e921-7298323c23d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"layer_normalization_60_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_layer_4_1/layer_normalization_60_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"layer_normalization_62_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_layer_5_1/layer_normalization_62_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"layer_normalization_64_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_layer_6_1/layer_normalization_64_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"layer_normalization_66_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_layer_7_1/layer_normalization_66_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"layer_normalization_68_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_layer_8_1/layer_normalization_68_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"layer_normalization_70_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_layer_9_1/layer_normalization_70_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_1/encoder_layer_4_1/layer_normalization_60_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_1/encoder_layer_5_1/layer_normalization_62_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_1/encoder_layer_6_1/layer_normalization_64_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_1/encoder_layer_7_1/layer_normalization_66_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_1/encoder_layer_8_1/layer_normalization_68_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "Tensor(\"encoder_1/encoder_layer_9_1/layer_normalization_70_1/add_2:0\", shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-1.506316    0.15101406  0.0492466   1.3060552 ]\n",
            "  [ 0.8889055   0.2238119   0.57073545 -1.683453  ]\n",
            "  [ 1.1112376   0.7489232  -0.44317025 -1.4169906 ]\n",
            "  [ 0.9864203  -1.4325075  -0.43854302  0.8846303 ]\n",
            "  [-1.5596968   0.73835564 -0.17451021  0.9958514 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-1.434469   -0.17873268  0.25472143  1.3584803 ]\n",
            "  [ 0.20656961  0.67477506  0.8070502  -1.6883948 ]\n",
            "  [ 1.0634995   0.7229256  -0.28055912 -1.5058658 ]\n",
            "  [ 1.433762   -1.2277379  -0.5589489   0.35292476]\n",
            "  [-1.584749    0.49306718 -0.02405709  1.1157391 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-1.4131548  -0.24848858  0.30133313  1.3603103 ]\n",
            "  [-0.23815049  0.86586714  0.9101807  -1.5378973 ]\n",
            "  [ 0.9733423   0.7670448  -0.1810741  -1.5593129 ]\n",
            "  [ 1.7153445  -0.7822542  -0.5384362  -0.39465407]\n",
            "  [-1.5627717   0.33617958  0.02488235  1.2017097 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-1.4336932  -0.23756388  0.33987898  1.3313781 ]\n",
            "  [-0.49006218  0.9828611   0.90954095 -1.4023398 ]\n",
            "  [ 0.6272733   0.77134407  0.3090288  -1.7076463 ]\n",
            "  [ 1.6404245  -0.18133272 -0.40423977 -1.0548521 ]\n",
            "  [-1.5777844   0.25197718  0.12988444  1.1959227 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-1.4774426  -0.16924371  0.35819256  1.2884938 ]\n",
            "  [-0.6325287   1.0430993   0.89643294 -1.3070034 ]\n",
            "  [ 0.28295246  0.8475702   0.56666625 -1.697189  ]\n",
            "  [ 1.3557007   0.3477703  -0.3084942  -1.3949769 ]\n",
            "  [-1.5778745   0.25463066  0.1277748   1.195469  ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-1.5272027  -0.07965045  0.37312627  1.233727  ]\n",
            "  [-0.74133414  1.0323343   0.9367124  -1.2277125 ]\n",
            "  [-0.10390653  0.929712    0.7668983  -1.5927037 ]\n",
            "  [ 1.0348531   0.66594297 -0.12955022 -1.5712458 ]\n",
            "  [-1.5816219   0.22770232  0.16214323  1.1917764 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-1.2361642   0.11574909 -0.39656657  1.5169815 ]\n",
            "  [ 1.010634    0.075082    0.5488355  -1.6345515 ]\n",
            "  [ 1.1849394   0.61105764 -0.3458572  -1.4501399 ]\n",
            "  [ 0.29282817 -1.3201499  -0.39291456  1.4202361 ]\n",
            "  [-0.90129507 -0.29266664 -0.49600568  1.6899674 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.9528561  -0.24675626 -0.47528452  1.674897  ]\n",
            "  [ 0.34265268  0.5930378   0.7757653  -1.711456  ]\n",
            "  [ 0.65037     1.0283055  -0.09414399 -1.5845314 ]\n",
            "  [ 0.8588155  -1.2979658  -0.64103234  1.0801827 ]\n",
            "  [-0.59613335 -0.56852454 -0.5672773   1.7319353 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.6781417  -0.5410081  -0.50941336  1.7285633 ]\n",
            "  [-0.14601041  0.8195706   0.90421575 -1.5777761 ]\n",
            "  [ 0.10900781  1.1978452   0.26835132 -1.5752045 ]\n",
            "  [ 1.2741556  -1.1599287  -0.772853    0.6586261 ]\n",
            "  [-0.36095607 -0.7643865  -0.5888982   1.7142408 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.45558044 -0.7551925  -0.5102171   1.7209902 ]\n",
            "  [-0.37758002  0.9713873   0.8730398  -1.4668471 ]\n",
            "  [-0.20640245  1.1794668   0.5364114  -1.5094757 ]\n",
            "  [ 1.5531912  -0.91456187 -0.84240496  0.20377567]\n",
            "  [-0.13194741 -0.90765095 -0.624398    1.6639965 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.24160062 -0.91480637 -0.52545357  1.6818606 ]\n",
            "  [-0.48297724  1.0414526   0.8446307  -1.4031061 ]\n",
            "  [-0.3826507   1.1484073   0.67578745 -1.4415439 ]\n",
            "  [ 1.664952   -0.5882617  -0.92709184 -0.14959848]\n",
            "  [ 0.11678927 -1.0259397  -0.6680397   1.57719   ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.01534013 -1.0465771  -0.5508466   1.6127636 ]\n",
            "  [-0.5725753   1.076234    0.84065413 -1.3443128 ]\n",
            "  [-0.50508845  1.1285738   0.7552245  -1.3787099 ]\n",
            "  [ 1.6331624  -0.26422927 -1.0881809  -0.2807522 ]\n",
            "  [ 0.3959545  -1.1160336  -0.72126293  1.4413419 ]]], shape=(1, 5, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0.01528832 0.03182636 0.01911612 0.01327814 0.01919508 0.02527708\n",
            "   0.04256717 0.02274482 0.02607786 0.01837319 0.01522125 0.02259313\n",
            "   0.01574946 0.02403509 0.01978756 0.04540539 0.06142665 0.02108401\n",
            "   0.03291011 0.03315888 0.02970277 0.01929083 0.0247163  0.01329596\n",
            "   0.04469232 0.03739361 0.03529752 0.04019889 0.05712175 0.0261829\n",
            "   0.0395442  0.04096638 0.01617936 0.01165208 0.03864947]\n",
            "  [0.02014697 0.04859223 0.02345644 0.03541204 0.0283726  0.04107721\n",
            "   0.03235343 0.0156891  0.01814077 0.02904724 0.06268603 0.02836419\n",
            "   0.03142245 0.01479187 0.01355626 0.02608576 0.02333027 0.01999775\n",
            "   0.05273187 0.02166903 0.04816905 0.03945329 0.02214636 0.06576873\n",
            "   0.0247204  0.01538066 0.01560687 0.0295563  0.015361   0.01498436\n",
            "   0.0255569  0.01383817 0.02658181 0.01545557 0.04049703]\n",
            "  [0.0195042  0.04954962 0.02321398 0.03477212 0.02767027 0.04047157\n",
            "   0.03330394 0.01576067 0.01843857 0.02860841 0.06189967 0.0286176\n",
            "   0.03057305 0.01498106 0.01368851 0.02659448 0.02388686 0.0199859\n",
            "   0.05336741 0.02167986 0.04909446 0.0390679  0.02228669 0.06428877\n",
            "   0.02514266 0.01537653 0.01598698 0.0297226  0.01560763 0.01496341\n",
            "   0.02575435 0.01385162 0.02600615 0.01516956 0.04111298]\n",
            "  [0.01899614 0.05044183 0.023175   0.03487456 0.0269051  0.03937114\n",
            "   0.03397715 0.01602066 0.01896625 0.0283564  0.06203504 0.02923188\n",
            "   0.02991881 0.01540039 0.01415678 0.02662026 0.02369985 0.02019717\n",
            "   0.05350977 0.02139918 0.05013032 0.03892028 0.02258017 0.06377792\n",
            "   0.0250932  0.01508282 0.01640141 0.02923789 0.0153769  0.01494244\n",
            "   0.02546671 0.01355465 0.02567461 0.01537614 0.04113113]\n",
            "  [0.00971088 0.05081175 0.0177044  0.01549489 0.01371065 0.02109294\n",
            "   0.05899794 0.02247601 0.03180029 0.01695094 0.02220656 0.03029742\n",
            "   0.01275546 0.02676486 0.02300554 0.04511683 0.05193997 0.02136672\n",
            "   0.0447165  0.02558787 0.04915224 0.02152133 0.02695585 0.01738761\n",
            "   0.0418411  0.02311889 0.03903257 0.0336702  0.03721017 0.02019881\n",
            "   0.03336421 0.02350566 0.01369229 0.01109676 0.04574382]]], shape=(1, 5, 35), dtype=float32)\n",
            "\u001b[92mAll tests passed\n"
          ]
        }
      ],
      "source": [
        "# UNIT TEST\n",
        "Transformer_test(Transformer, create_look_ahead_mask, create_padding_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lthsC3cT_4IK"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook explored the inner workings of the Transformer architecture by implementing it from the ground up. Key components such as positional encoding, self-attention, and multi-head attention were developed and integrated into full encoder and decoder modules. Through this process, the final model closely mirrors the original Transformer architecture introduced by Vaswani et al. and provides a foundation for further experimentation with advanced NLP tasks.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Self-Attention** enables each token to dynamically focus on other tokens in the sequence, regardless of position.\n",
        "- **Multi-Head Attention** improves the model’s ability to capture different contextual relationships by learning multiple attention distributions in parallel.\n",
        "- **Positional Encoding** injects order information into input embeddings, which is crucial for understanding sequential data in a parallel architecture.\n",
        "- **Residual Connections and Layer Normalization** help stabilize training and preserve gradient flow in deep architectures.\n",
        "- **Masking** allows the model to ignore padded tokens or restrict future token access during training (causal masking).\n",
        "\n",
        "The resulting architecture is highly parallelizable and good for use in modern NLP tasks.\n",
        "\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "With the core Transformer structure implemented, this model can now be extended and applied to real-world problems such as:\n",
        "\n",
        "- **Named Entity Recognition (NER)**\n",
        "- **Extractive Question Answering**\n",
        "- **Text Summarization**\n",
        "- **Sequence Classification**\n",
        "\n",
        "The encoder and decoder modules developed here can be fine-tuned or enhanced for these applications in downstream notebooks.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017).  \n",
        "  [Attention Is All You Need](https://arxiv.org/abs/1706.03762). *Advances in Neural Information Processing Systems*.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
